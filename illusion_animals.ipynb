{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When humans encounter optical illusions, our brains often see things that aren't physically present in the image. \n",
    "\n",
    "This perceptual phenomenon, known as pareidolia, has long fascinated neuroscientists and psychologists. Now, researchers are turning these visual puzzles toward Vision-Language Models (VLM) to test their perceptual capabilities.\n",
    "\n",
    "I recently came across a paper,*[Illusory VQA: Benchmarking and Enhancing Multimodal Models on Visual Illusions\n",
    "](https://arxiv.org/abs/2412.08169)*, which introduces a novel task called Illusory VQA. The core challenge presented in the [Illusory VQA](https://github.com/IllusoryVQA/IllusoryVQA) task is deceptively complex: given an image containing both a \"Real Concept\" (RC) and potentially an \"Illusory Concept\" (IC), can a VLM detect if an illusion is present and correctly answer questions about that illusory element?\n",
    "\n",
    "This task requires perception beyond standard image recognition and assessing how well models can mimic human-like visual understanding, and is interestingly challenging because the model must simultaneously recognize what's actually in the image while also perceiving what appears to be there due to the illusion — much like our own visual system does.\n",
    "\n",
    "### The Illusory Datasets\n",
    "\n",
    "For this task, the authors created four benchmark datasets, each targeting different aspects of visual illusion processing:\n",
    "\n",
    "• **IllusionMNIST:** Built using the classic MNIST handwritten digit dataset as the source material, this dataset contains 3,960 training samples and 1,219 test samples. The researchers added a \"No illusion\" class to make the task more challenging, requiring models to determine whether an illusion is actually present.\n",
    "\n",
    "• **IllusionFashionMNIST:** Based on the Fashion-MNIST dataset, which contains clothing items rather than digits, this collection includes 3,300 training samples and 1,267 test samples. Like its MNIST counterpart, it includes a \"No illusion\" class to further test discrimination abilities.\n",
    "\n",
    "• **IllusionAnimals:** This dataset features animal images generated using SDXL-Lightning and transformed with ControlNet to create illusory versions. It comprises 3,300 training samples and 1,100 test samples, with the additional \"No illusion\" class.\n",
    "\n",
    "• **IllusionChar:** This unique dataset focuses specifically on reading characters in images, with sequences of 3 to 5 characters per image. Including 9,900 training samples and 3,300 test samples, it tests how well models can interpret text within illusory contexts.\n",
    "\n",
    "What I found particularly interesting is how these datasets were constructed:\n",
    "\n",
    "<img src=\"assets/illusoryvqa-datagen.png\" width=\"70%\">\n",
    "\n",
    "The research team:\n",
    "\n",
    "- Generated scene descriptions using large language models\n",
    "\n",
    "- Combined these descriptions with raw images\n",
    "\n",
    "- Used a variant of ControlNet to create the final illusory images\n",
    "\n",
    "- Conducted human evaluations to validate the quality of the generated images\n",
    "\n",
    "- Asked participants to identify what they perceived in each picture\n",
    "\n",
    "- Filtered out inappropriate content using NSFW detectors\n",
    "\n",
    "This approach ensures that the illusions in the datasets genuinely challenge perceptual abilities in ways that mirror human visual processing.\n",
    "\n",
    "### Testing Leading Multimodal Models\n",
    "\n",
    "The study evaluated several state-of-the-art models:\n",
    "\n",
    "<img src=\"assets/illusoryvqa-table2.png\" width=\"70%\">\n",
    "\n",
    "The research team focused on zero-shot performance (how well models perform without specific training on illusions) and performance after fine-tuning.\n",
    "\n",
    "The results that all models showed a performance drop when dealing with illusions compared to standard images—mirroring the human experience of being \"fooled\" by optical illusions. Different models demonstrated varying levels of robustness to different types of illusions, suggesting that architectural differences influence how these systems process visual information.\n",
    "\n",
    "### A Simple Yet Effective Solution\n",
    "\n",
    "An interesting finding from the research is their straightforward solution for improving model performance on illusory images. The technique:\n",
    "\n",
    "1. Apply a Gaussian and blur low-pass filter to the illusory images\n",
    "2. Convert the images to grayscale\n",
    "\n",
    "This simple preprocessing approach yielded significant performance improvements across all tested models. \n",
    "\n",
    "For example, in the IllusionAnimals dataset:\n",
    "\n",
    "- CLIP initially showed the highest performance on illusory images\n",
    "\n",
    "- After applying the filter, BLIP-2 achieved the best results—even outperforming humans\n",
    "\n",
    "- All models saw substantial gains in accuracy after implementing the filtering technique\n",
    "\n",
    "This finding suggests that relatively simple image processing techniques can help AI systems overcome perceptual challenges posed by illusions. The filtering process essentially helps the models differentiate between real and illusory elements in the images, similar to how certain visual processing aids might help humans see through optical illusions.\n",
    "\n",
    "## What we're going to do in this tutorial\n",
    "\n",
    "\n",
    "In this tutorial, we'll explore the IllusionAnimals dataset and evaluate how different AI models perceive visual illusions. We'll:\n",
    "\n",
    "1. Load and explore the IllusionAnimals dataset using FiftyOne\n",
    "\n",
    "2. Compute embeddings using multiple state-of-the-art models:\n",
    "   - SigLIP (by Google)\n",
    "   - AIM-v2 (by Apple)\n",
    "\n",
    "3. Visualize these embeddings using UMAP dimensionality reduction\n",
    "\n",
    "4. Perform zero-shot classification using various models\n",
    "\n",
    "5. Test Visual Question-Answering (VQA) capabilities using:\n",
    "   - Janus-Pro\n",
    "   - Moondream2\n",
    "\n",
    "6. Compare how models perform with and without hints about potential illusions\n",
    "\n",
    "Let's start by installing some dependencies and downloading the dataset from the Hugging Face Hub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/huggingface/transformers.git#egg=transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fiftyone umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "from fiftyone.utils.huggingface import load_from_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_hub(\n",
    "    \"harpreetsahota/IllusionAnimals\",\n",
    "    overwrite=True,\n",
    "    persistent=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with an initial visual vibe check of what is in this dataset.\n",
    "\n",
    "Note this is a [Grouped Dataset](https://docs.voxel51.com/user_guide/groups.html#grouped-datasets). Grouped datasets allow us to represent multiples slices of the same data point. This way data for multiple perspectives of the same scene can be stored, visualized, and queried in ways that respect the relationships between the slices of data.\n",
    "\n",
    "For the IllusionAnimals subset, the dataset includes different slices representing variations of the images with and without illusions, and with and without filters. The specific slices available in the IllusionAnimals dataset are:\n",
    "\n",
    "*   **Raw Images**: These are the original images of animals without any illusions applied. They serve as a baseline for evaluating the models' performance on standard image recognition tasks. The models should accurately identify the animal in the image.\n",
    "\n",
    "*   **Illusory Images**: These images have visual illusions incorporated into them. The illusions are designed to make the images appear as one animal while subtly containing elements of another. The goal is to test whether the models can detect the presence of the illusory concept, even with the presence of the real concept.\n",
    "\n",
    "*   **Filtered Images**: These are the illusory images that have been processed with a Gaussian and blur low-pass filter. This filter is applied to enhance the models’ ability to detect the illusions. The idea is that the filter helps to reduce noise and highlight the illusory elements, making it easier for the models to identify and interpret the content. Applying the filter generally improves model performance.\n",
    "\n",
    "*   **Illusionless Class**: In addition to the above, an extra class called \"illusionless\" is added to push the models’ capabilities. This class enables the models to detect instances where no illusion images are present in the picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = dataset.distinct(\"label.label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll set an enviornment variable as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['FIFTYONE_ALLOW_LEGACY_ORCHESTRATORS'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download https://github.com/harpreetsahota204/aimv2_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins requirements @harpreetsahota/aimv2_embeddings --install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download https://github.com/harpreetsahota204/hiera-embeddings-plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins requirements @harpreetsahota/hiera_embeddings --install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "siglip_model = foz.load_zoo_model(\n",
    "    \"zero-shot-classification-transformer-torch\",\n",
    "    name_or_path=\"google/siglip2-base-patch16-512\", \n",
    "    classes=class_names,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.compute_embeddings(\n",
    "    model=siglip_model,\n",
    "    embeddings_field=\"siglip_emb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.operators as foo\n",
    "\n",
    "aim_embeddings = foo.get_operator(\"@harpreetsahota/aimv2_embeddings/compute_aimv2_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the operator on your dataset\n",
    "await aim_embeddings(\n",
    "    dataset,\n",
    "    model_name=\"apple/aimv2-large-patch14-224\",  # Choose any supported model\n",
    "    embedding_types=\"mean\",\n",
    "    emb_field=\"aimv2_mean_embeddings\",\n",
    "    delegate=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the operator on your dataset\n",
    "await aim_embeddings(\n",
    "    dataset,\n",
    "    model_name=\"apple/aimv2-large-patch14-224\",  # Choose any supported model\n",
    "    embedding_types=\"cls\",\n",
    "    emb_field=\"aimv2_cls_embeddings\",\n",
    "    delegate=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.brain as fob\n",
    "\n",
    "embedding_fields = [ \n",
    "    \"aimv2_mean_embeddings\",\n",
    "    \"aimv2_cls_embeddings\",\n",
    "    \"siglip_emb\"\n",
    "    ]\n",
    "\n",
    "for fields in embedding_fields:\n",
    "    _fname = fields.split(\"_embeddings\")[0]\n",
    "    results = fob.compute_visualization(\n",
    "        dataset,\n",
    "        embeddings=fields,\n",
    "        method=\"umap\",\n",
    "        brain_key=f\"{_fname}_viz\",\n",
    "        num_dims=2,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot classification using Siglip and aimv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.apply_model(\n",
    "    model=siglip_model, \n",
    "    label_field=\"siglip2_predictions\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aimv2_model = foz.load_zoo_model(\n",
    "    \"zero-shot-classification-transformer-torch\",\n",
    "    name_or_path=\"apple/aimv2-large-patch14-224-lit\", \n",
    "    classes=class_names,\n",
    "    trust_remote_code=True,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.apply_model(\n",
    "    model=aimv2_model, \n",
    "    label_field=\"aimv2_predictions\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate classifications and see the results\n",
    "\n",
    "\n",
    "### Can VLMs do any better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download https://github.com/harpreetsahota204/janus-vqa-fiftyone\n",
    "\n",
    "!fiftyone plugins requirements @harpreetsahota/janus_vqa --install\n",
    "\n",
    "!fiftyone plugins download https://github.com/harpreetsahota204/moondream2-plugin\n",
    "\n",
    "!fiftyone plugins requirements @harpreetsahota/moondream2 --install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_HINT_PROMPT = f\"\"\"Which class is in the picture: {', '.join(class_names)}. \n",
    "Your answer must be one of these exact classes, no other answers allowed. \n",
    "Respond in one word for your guess of the correct class without any extra explanation.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.operators as foo\n",
    "\n",
    "janus_vqa = foo.get_operator(\"@harpreetsahota/janus_vqa/janus_vqa\")\n",
    "\n",
    "moondream = foo.get_operator(\"@harpreetsahota/moondream2/moondream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No hint prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "await janus_vqa(\n",
    "    dataset,\n",
    "    model_path=\"deepseek-ai/Janus-Pro-1B\",\n",
    "    question=NO_HINT_PROMPT,\n",
    "    question_field=\"no_hint_prompt\",\n",
    "    answer_field=\"janus_no_hint_answer\",\n",
    "    delegate=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await moondream(\n",
    "    dataset,\n",
    "    revision=\"2025-01-09\",\n",
    "    operation=\"query\",\n",
    "    output_field=\"moondream_no_hint_answer\",\n",
    "    query_text=NO_HINT_PROMPT,\n",
    "    delegate=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "HINT_PROMPT = f\"\"\"There might be an image illusion of something in this image. \n",
    "These are the classes that the image illusion might belong to: {', '.join(class_names)}.\n",
    "Your answer must be one of these exact classes, no other answers allowed.  \n",
    "Respond in one word for your guess of the correct class without any extra explanation.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "await janus_vqa(\n",
    "    dataset,\n",
    "    model_path=\"deepseek-ai/Janus-Pro-1B\",\n",
    "    question=HINT_PROMPT,\n",
    "    question_field=\"hint_prompt\",\n",
    "    answer_field=\"janus_hint_answer\",\n",
    "    delegate=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await moondream(\n",
    "    dataset,\n",
    "    revision=\"2025-01-09\",\n",
    "    operation=\"query\",\n",
    "    output_field=\"moondream_hint_answer\",\n",
    "    query_text=HINT_PROMPT,\n",
    "    delegate=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moondream2 also produces short captions, let's generate short captions and then compute similarity between the caption and the ground truth prompt\n",
    "\n",
    "Then let's also see if any of the captions actually include the classes of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why This Matters for AI Practitioners and Researchers\n",
    "\n",
    "This research opens exciting new avenues for improving multimodal AI systems:\n",
    "\n",
    "1. **Perceptual Robustness**: Training models to handle illusory images could make them more robust to adversarial attacks and unusual visual inputs. If a model can correctly process information even when presented with potentially misleading visual cues, it may be less susceptible to manipulation or confusion in real-world applications.\n",
    "\n",
    "2. **Cognitive Alignment**: Understanding how AI models perceive illusions differently from humans can help researchers better align AI visual processing with human cognition. This alignment is crucial for applications where AI systems need to interpret visual information similarly to humans, such as in autonomous driving or medical image analysis.\n",
    "\n",
    "3. **Preprocessing Solutions**: The simple filtering technique offers an immediate way to improve model performance on challenging visual inputs without requiring extensive retraining or architectural changes.\n",
    "\n",
    "4. **Benchmark Advancement**: The Illusory VQA datasets provide valuable new benchmarks that push beyond conventional image recognition tasks, helping researchers identify strengths and weaknesses in current multimodal architectures.\n",
    "\n",
    "5. **Bridging Disciplines**: This work creates interesting connections between AI research and cognitive psychology, potentially leading to cross-disciplinary insights about visual perception.\n",
    "\n",
    "For AI practitioners working on multimodal systems, incorporating illusion testing into evaluation protocols could reveal important limitations that might otherwise go undetected. Similarly, the preprocessing techniques described in this research could be adapted for a variety of challenging visual inputs beyond just illusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "fiftyone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
