{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When humans encounter optical illusions, our brains often see things that aren't physically present in the image.  This perceptual phenomenon, known as pareidolia, has long fascinated neuroscientists and psychologists. Now, researchers are turning these visual puzzles toward Vision-Language Models (VLM) to test their perceptual capabilities.\n",
    "\n",
    "I recently came across a paper, *[Illusory VQA: Benchmarking and Enhancing Multimodal Models on Visual Illusions\n",
    "](https://arxiv.org/abs/2412.08169)*, which introduces a novel task called Illusory VQA. \n",
    "\n",
    "The core challenge presented in the [Illusory VQA](https://github.com/IllusoryVQA/IllusoryVQA) task is deceptively complex: given an image containing both a \"Real Concept\" (RC) and potentially an \"Illusory Concept\" (IC), can a VLM detect if an illusion is present and correctly answer questions about that illusory element?\n",
    "\n",
    "This task requires perception beyond standard image recognition and assessing how well models can mimic human-like visual understanding, and is interestingly challenging because the model must simultaneously recognize what's actually in the image while also perceiving what appears to be there due to the illusion — much like our own visual system does.\n",
    "\n",
    "### The Illusory Datasets\n",
    "\n",
    "For this task, the authors created four benchmark datasets, each targeting different aspects of visual illusion processing:\n",
    "\n",
    "• **IllusionMNIST:** Built using the classic MNIST handwritten digit dataset as the source material, this dataset contains 3,960 training samples and 1,219 test samples. The researchers added a \"No illusion\" class to make the task more challenging, requiring models to determine whether an illusion is actually present.\n",
    "\n",
    "• **IllusionFashionMNIST:** Based on the Fashion-MNIST dataset, which contains clothing items rather than digits, this collection includes 3,300 training samples and 1,267 test samples. Like its MNIST counterpart, it includes a \"No illusion\" class to further test discrimination abilities.\n",
    "\n",
    "• **IllusionAnimals:** This dataset features animal images generated using SDXL-Lightning and transformed with ControlNet to create illusory versions. It comprises 3,300 training samples and 1,100 test samples, with the additional \"No illusion\" class.\n",
    "\n",
    "• **IllusionChar:** This unique dataset focuses specifically on reading characters in images, with sequences of 3 to 5 characters per image. Including 9,900 training samples and 3,300 test samples, it tests how well models can interpret text within illusory contexts.\n",
    "\n",
    "What I found particularly interesting is how these datasets were constructed:\n",
    "\n",
    "<img src=\"assets/illusoryvqa-datagen.png\" width=\"70%\">\n",
    "\n",
    "The research team:\n",
    "\n",
    "- Generated scene descriptions using large language models\n",
    "\n",
    "- Combined these descriptions with raw images\n",
    "\n",
    "- Used a variant of ControlNet to create the final illusory images\n",
    "\n",
    "- Conducted human evaluations to validate the quality of the generated images\n",
    "\n",
    "- Asked participants to identify what they perceived in each picture\n",
    "\n",
    "- Filtered out inappropriate content using NSFW detectors\n",
    "\n",
    "This approach ensures that the illusions in the datasets genuinely challenge perceptual abilities in ways that mirror human visual processing.\n",
    "\n",
    "### Testing Leading Multimodal Models\n",
    "\n",
    "The study evaluated several state-of-the-art models:\n",
    "\n",
    "<img src=\"assets/illusoryvqa-table2.png\" width=\"70%\">\n",
    "\n",
    "The research team focused on zero-shot performance (how well models perform without specific training on illusions) and performance after fine-tuning.\n",
    "\n",
    "The results that all models showed a performance drop when dealing with illusions compared to standard images—mirroring the human experience of being \"fooled\" by optical illusions. Different models demonstrated varying levels of robustness to different types of illusions, suggesting that architectural differences influence how these systems process visual information.\n",
    "\n",
    "### A Simple Yet Effective Solution\n",
    "\n",
    "An interesting finding from the research is their straightforward solution for improving model performance on illusory images. The technique:\n",
    "\n",
    "1. Apply a Gaussian and blur low-pass filter to the illusory images\n",
    "2. Convert the images to grayscale\n",
    "\n",
    "This simple preprocessing approach yielded significant performance improvements across all tested models. \n",
    "\n",
    "For example, in the IllusionAnimals dataset:\n",
    "\n",
    "- CLIP initially showed the highest performance on illusory images\n",
    "\n",
    "- After applying the filter, BLIP-2 achieved the best results—even outperforming humans\n",
    "\n",
    "- All models saw substantial gains in accuracy after implementing the filtering technique\n",
    "\n",
    "This finding suggests that relatively simple image processing techniques can help AI systems overcome perceptual challenges posed by illusions. The filtering process essentially helps the models differentiate between real and illusory elements in the images, similar to how certain visual processing aids might help humans see through optical illusions.\n",
    "\n",
    "## What we're going to do in this tutorial\n",
    "\n",
    "In this tutorial, we'll explore the IllusionAnimals dataset and evaluate how different AI models perceive visual illusions. We'll:\n",
    "\n",
    "1. Load and explore the IllusionAnimals dataset using FiftyOne and see if we can reproduce the results from the paper, but only focusing on the CLIP model.\n",
    "\n",
    "2. Compute embeddings using multiple models:\n",
    "\n",
    "   - CLIP  \n",
    "\n",
    "   - SigLIP 2 (a new model released by Google)\n",
    "   \n",
    "   - AIMv2 (in my opinion a highly slept on contender to CLIP released in late 2024 by Apple)\n",
    "\n",
    "3. Visualize these embeddings using UMAP dimensionality reduction\n",
    "\n",
    "4. Perform zero-shot classification using the models mentioned above\n",
    "\n",
    "5. Test Visual Question-Answering (VQA) capabilities using:\n",
    "   - Janus-Pro\n",
    "   - Moondream2\n",
    "\n",
    "6. Compare how models perform with and without hints about potential illusions\n",
    "\n",
    "Let's start by installing some dependencies and downloading the dataset from the Hugging Face Hub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing bleeding edge version of transformers\n",
    "!pip install git+https://github.com/huggingface/transformers.git#egg=transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fiftyone umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "from fiftyone.utils.huggingface import load_from_hub\n",
    "\n",
    "dataset = load_from_hub(\n",
    "    \"Voxel51/IllusionAnimals\",\n",
    "    overwrite=True,\n",
    "    persistent=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with an initial visual vibe check of what is in this dataset.\n",
    "\n",
    "<img src=\"assets/illusion_animals_initial_explore.gif\" width=\"70%\">\n",
    "\n",
    "\n",
    "Note this is a [Grouped Dataset](https://docs.voxel51.com/user_guide/groups.html#grouped-datasets). Grouped datasets allow us to represent multiples slices of the same data point. This way data for multiple perspectives of the same scene can be stored, visualized, and queried in ways that respect the relationships between the slices of data.\n",
    "\n",
    "For the IllusionAnimals subset, the dataset includes different slices representing variations of the images with and without illusions, and with and without filters. The specific slices available in the IllusionAnimals dataset are:\n",
    "\n",
    "*   **Raw Images**: These are the original images of animals without any illusions applied. They serve as a baseline for evaluating the models' performance on standard image recognition tasks. The models should accurately identify the animal in the image.\n",
    "\n",
    "*   **Illusory Images**: These images have visual illusions incorporated into them. The illusions are designed to make the images appear as one animal while subtly containing elements of another. The goal is to test whether the models can detect the presence of the illusory concept, even with the presence of the real concept.\n",
    "\n",
    "*   **Filtered Images**: These are the illusory images that have been processed with a Gaussian and blur low-pass filter. This filter is applied to enhance the models’ ability to detect the illusions. The idea is that the filter helps to reduce noise and highlight the illusory elements, making it easier for the models to identify and interpret the content. Applying the filter generally improves model performance.\n",
    "\n",
    "*   **Illusionless Class**: In addition to the above, an extra class called \"illusionless\" is added to push the models’ capabilities. This class enables the models to detect instances where no illusion images are present in the picture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're only going to work with two of the slices in this tutorial, \"main\" (the illusion and no illusion images) and \"filtered\" (which are the images after the Gaussian Blur and grayscaling).\n",
    "\n",
    "FiftyOne datasets are logical datasets pointing to media files on disk rather than storing the media file contents directly. So by cloning the dataset we are not duplicating the images on disk only the schema.\n",
    "\n",
    "Note that when I originally parsed the dataset I mapped the images with no illusion to the `illusionless` class. To be consisent with the paper, I'm going to map these to the `no illusion` class. I was too lazy to reparse the dataset, but luckily this is easy to do in FiftyOne using the `map_labels` method of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_images = dataset.select_group_slices(\"main\").map_labels(\"label\", {\"illusionless\": \"no illusion\"}).clone(name=\"illusion_animals\") # get the main images\n",
    "main_images.persistent = True # make the dataset persistent across sessions \n",
    "\n",
    "filtered_images = dataset.select_group_slices(\"filtered\").map_labels(\"label\", {\"illusionless\": \"no illusion\"}).clone(name=\"illusion_animals_fitered\")\n",
    "filtered_images.persistent = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also need to have the labels, so we can grab them now. It doesn't matter which slice we grab them from as they are both the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = main_images.distinct(\"label.label\") # get the class names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Embeddings for Deeper Dataset Understanding\n",
    "\n",
    "The first thing I want to do is gain a deeper understanding of the images in this dataset, for that we can use embeddings.\n",
    "\n",
    "Visual embeddings are high-dimensional vector representations of images that capture semantic and visual features. For the IllusionAnimals dataset, embeddings are particularly valuable because they can help us:\n",
    "\n",
    "1. **Visualize Relationships**: Reducing these high-dimensional embeddings to 2D using UMAP helps visualize how different images cluster together and potentially identify patterns in the dataset.\n",
    "\n",
    "2. **Compare Model Perspectives**: Different models may encode visual information differently. By comparing embeddings from multiple models (SigLIP and AIM-v2 in our case), we can understand how their \"perception\" of illusions differs.\n",
    "\n",
    "3. **Analyze Illusion Effects**: We can examine whether illusory versions of images cluster closer to their \"real\" concept or their \"illusory\" concept, giving us insights into how effectively the illusions work from a model's perspective.\n",
    "\n",
    "For this analysis, we'll use three models:\n",
    "\n",
    "- **CLIP**\n",
    "\n",
    "- **SigLIP 2** \n",
    "\n",
    "- **AIM-v2** \n",
    "\n",
    "Let's start by instantiating the models and then computing embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CLIP\n",
    "\n",
    "There's been a lot written about the CLIP model, so I won't repeat anything here. However, if you're interested in going deep into CLIP and its history then [check out this blog](https://voxel51.com/blog/a-history-of-clip-model-training-data-advances/).\n",
    "\n",
    "We can use the model via [FiftyOne's integration with Hugging Face](https://docs.voxel51.com/integrations/huggingface.html#zero-shot-classification). In the paper the authors use `CLIP-ViT-base-patch32` in their experiments, which is the checkpoint we will also use. You'll see that we instantiate the model with classes but it will not affect the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "clip_model = foz.load_zoo_model(\n",
    "    \"zero-shot-classification-transformer-torch\",\n",
    "    name_or_path=\"openai/clip-vit-base-patch32\", \n",
    "    classes=class_names,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    # install_requirements=True # uncomment this line if you are running this code for the first time\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_images.compute_embeddings(\n",
    "    model=clip_model,\n",
    "    embeddings_field=\"clip_embeddings\"\n",
    ")\n",
    "\n",
    "filtered_images.compute_embeddings(\n",
    "    model=clip_model,\n",
    "    embeddings_field=\"clip_embeddings\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SigLIP 2\n",
    "\n",
    "SigLIP 2 is a [family of multilingual vision-language encoders](https://huggingface.co/collections/google/siglip2-67b5dcef38c175486e240107) that improves upon the original SigLIP model. It incorporates several techniques, including captioning-based pretraining, self-supervised losses (self-distillation, masked prediction), and online data curation, into a unified training recipe.\n",
    "\n",
    "I don't want to get too deep into the technical details of the SigLIP 2 model, feel free to [check out the paper](https://arxiv.org/abs/2502.14786) for more information about the model, it's performance, and how it was trained.\n",
    "\n",
    "##### SigLIP 2 Data Curation\n",
    "\n",
    "What I do want to spend some time talking about, however, is the data curation methods discussed in the paper. The data curation method in focuses on improving the quality and diversity of training data, especially for smaller models. The primary technique used is **[Active Curation as Implicit Distillation (ACID)](https://arxiv.org/pdf/2411.18674)**. Here's a breakdown of how it works:\n",
    "\n",
    "1.  **Teacher-Student Model Setup**:\n",
    "    *   A smaller model (student) is trained with the help of a more powerful, pre-trained model (teacher).\n",
    "\n",
    "    *   In SigLIP 2, the **teacher model is a fine-tuned SigLIP 2 So400m model**, which is initially trained on a diverse dataset and then further fine-tuned on a high-quality curated dataset.\n",
    "\n",
    "2.  **Scoring Examples for \"Learnability\"**:\n",
    "\n",
    "    *   During training, both the teacher and student models evaluate the training examples.\n",
    "\n",
    "    *   Each example is scored based on how \"learnable\" it is. This score is basically the difference in loss values between the current student model and the reference model, and is meant to reflect how well the model can learn from that particular example.\n",
    "\n",
    "     *  Examples that are easy for the reference but difficult for the current student are given high scores. This means the data is \"learnable\" because the reference model, which has already been trained, can easily understand the patterns in the data. The student model, still in training, finds these patterns more challenging.\n",
    "\n",
    "3.  **Active Sample Selection**:\n",
    "\n",
    "    *   Instead of using all available data, **only the most \"learnable\" examples are selected** for each training batch.\n",
    "\n",
    "    *   This selection is done jointly, with two main criteria for scoring sub-batches:\n",
    "\n",
    "        *   Easy-reference scoring: This uses the loss values of the reference model to preferentially sample batches that are easy for the reference model.\n",
    "\n",
    "        *   Learnability scoring: This uses the difference in loss values between the current student model and the reference model to prioritize batches that are easy for the reference model but difficult for the student model. Batches that are easy for the reference but difficult for the current student are given high scores\n",
    "\n",
    "    *   By curating data based on the reference model, ACID implicitly distills its knowledge through a data-driven objective. This objective combines model predictions and real labels as targets, and retains targets where the reference model and labels agree allowing for mutual denoising of model predictions and data labels\n",
    "\n",
    "4.  **Filtering Ratio**:\n",
    "    *   To balance the benefits of curation with computational costs, a filtering ratio is applied.\n",
    "\n",
    "    *   For example, a filtering ratio of 0.5 means that the super-batch is twice the size of the final batch (64,000 examples), and the best 50% are selected.\n",
    "\n",
    "    *   The B/32 model uses a filtering ratio of 0.75.\n",
    "\n",
    "5.  **Implicit Distillation**:\n",
    "\n",
    "    *   By selectively training on the most informative examples, the student model (the smaller SigLIP 2 model) learns to mimic the behavior of the teacher model.\n",
    "\n",
    "    *   This process implicitly distills the knowledge from the larger, more capable teacher model into the smaller student model, improving its performance.\n",
    "\n",
    "    *   This method saves computational resources compared to explicit distillation methods that may use a second teacher model.\n",
    "\n",
    "In essence, this data curation method ensures that the smaller models are trained on the most valuable data, leading to improved performance and efficiency. By fine-tuning the teacher model on a curated dataset, the method also captures the benefits of diverse knowledge and high-quality data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "siglip_model = foz.load_zoo_model(\n",
    "    \"zero-shot-classification-transformer-torch\",\n",
    "    name_or_path=\"google/siglip2-base-patch32-256\",\n",
    "    classes=class_names,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    # install_requirements=True # uncomment this line if you are running this code for the first time\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note, I'm using this particular checkpoint because I'm trying to be as \"apples-to-apples\" with the CLIP model as possible. And, of course, these are two completely different model architectures and trained on completely different datasets. And, for the purposes of this tutorial, \"apples-to-apples\" means picking a model that have fairly close names. In this case, they both use `ViT/B 32` as the vision encoder...that's good enough for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_images.compute_embeddings(\n",
    "    model=siglip_model,\n",
    "    embeddings_field=\"siglip_embeddings\"\n",
    ")\n",
    "\n",
    "filtered_images.compute_embeddings(\n",
    "    model=siglip_model,\n",
    "    embeddings_field=\"siglip_embeddings\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AIMv2\n",
    "\n",
    "AIMv2 is a [family of vision encoders released in late 2024](huggingface.co/collections/apple/aimv2-6720fe1558d94c7805f7688c) that uses a novel multimodal autoregressive method. \n",
    "\n",
    "It processes image patches and text tokens as a unified sequence, using a causal multimodal decoder to predict elements sequentially. AIMv2 processes data as one continuous sequence, predicting the next step in the series. It deliberately puts image information first, followed by text, creating a specific sequence: image patches → text tokens. This differs from CLIP's parallel processing of image and text and strengthens the vision encoder. AIMv2 is trained on 12 billion image-text samples, balancing human-written alt-text and synthetically generated captions from diverse sources. I've written about the AIMv2 models in great detail in two blog posts, which you can read [here](https://medium.com/voxel51/visual-understanding-with-aimv2-76c58dcd68f9) and [here](https://medium.com/voxel51/aimv2-outperforms-clip-on-synthetic-dataset-imagenet-d-4452760b624c).\n",
    "\n",
    "In order to use AIMv2 for embeddings, we need to [install a plugin](https://github.com/harpreetsahota204/aim-embeddings-plugin)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download https://github.com/harpreetsahota204/aimv2_embeddings\n",
    "\n",
    "!fiftyone plugins requirements @harpreetsahota/aimv2_embeddings --install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to set an enviornment variable as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['FIFTYONE_ALLOW_LEGACY_ORCHESTRATORS'] = 'true'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll also need to kick off a delegated service by running `fiftyone delegated launch` in the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.operators as foo\n",
    "\n",
    "aim_embeddings = foo.get_operator(\"@harpreetsahota/aimv2_embeddings/compute_aimv2_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the operator on your dataset\n",
    "await aim_embeddings(\n",
    "    main_images,\n",
    "    model_name=\"apple/aimv2-large-patch14-224\",  # Choose any supported model\n",
    "    embedding_types=\"cls\", #can be \"cls\", \"mean\"\n",
    "    emb_field=\"aimv2_embeddings\",\n",
    "    delegate=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the operator on your dataset\n",
    "await aim_embeddings(\n",
    "    filtered_images,\n",
    "    model_name=\"apple/aimv2-large-patch14-224\",  # Choose any supported model\n",
    "    embedding_types=\"cls\",\n",
    "    emb_field=\"aimv2_embeddings\",\n",
    "    delegate=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring embeddings\n",
    "\n",
    "Now we can use UMAP to reduce the dimensionality of the embeddings and explore them in the FiftyOne app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.brain as fob\n",
    "\n",
    "# Define datasets and embedding fields as lists\n",
    "datasets = [main_images, filtered_images]\n",
    "embedding_fields = [\n",
    "    \"aimv2_embeddings\",\n",
    "    \"clip_embeddings\",\n",
    "    \"siglip_embeddings\"\n",
    "]\n",
    "\n",
    "# Compute UMAP for each dataset and embedding combination\n",
    "for ds in datasets:\n",
    "    for field in embedding_fields:\n",
    "        _fname = field.split(\"_embeddings\")[0]\n",
    "        brain_key = f\"{_fname}_viz\"\n",
    "        \n",
    "        results = fob.compute_visualization(\n",
    "            ds,\n",
    "            embeddings=field,\n",
    "            method=\"umap\",\n",
    "            brain_key=brain_key,\n",
    "            num_dims=2,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(main_images)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"assets/illusion-embeddings.gif\" width=\"70%\">\n",
    "\n",
    "Looking at the embeddings between filtered and non-filtered images, there are several observations:\n",
    "\n",
    "1. **Embedding Separation**\n",
    "\n",
    "- The filtered images (after Gaussian blur and grayscale conversion) tend to form tighter, more distinct clusters compared to their unfiltered counterparts. This is especially noticible in the AIMv2 and CLIP embedding spaces. Notably the SigLIP 2 model doesn't show much clustering and the embeddings look uniformly distributed across the embedding space. You'll likely see different results for a different check point, but this particular checkpoint doesn't seem to discern classes that well.\n",
    "\n",
    "- This suggests the filtering process helps the models create more consistent representations of similar images.\n",
    "\n",
    "2. **Noise Reduction**\n",
    "\n",
    "- The filtered embeddings show less scatter/noise, indicating that the preprocessing helps reduce irrelevant visual variations; the notable exception being the SigLIP 2 embeddings.\n",
    "\n",
    "- This aligns with the paper's findings that filtering improves model performance by helping models focus on the essential features that define the illusions.\n",
    "\n",
    "3. **Class Boundary Clarity**\n",
    "\n",
    "- In the filtered version, the boundaries between different classes appear more defined.\n",
    "\n",
    "- This is particularly noticeable for the \"no illusion\" class, which forms more cohesive clusters after filtering.\n",
    "\n",
    "4. **Distance Between Related Concepts**\n",
    "\n",
    "- The filtered embeddings seem to better capture the relationship between the real concept and illusory concept, as evidenced by more logical spatial relationships in the embedding space.\n",
    "\n",
    "- This suggests the filtering process helps models better understand both the actual and illusory elements in the images.\n",
    "\n",
    "This visualization helps explain why the simple preprocessing technique (Gaussian blur + grayscale) improved model performance in the original paper - it's creating cleaner, more structured representations that are easier for the models to work with.\n",
    "\n",
    "### Computing uniquess and representativeness values\n",
    "\n",
    "We can use the embeddings to compute uniqueness values for the images in our dataset. \n",
    "\n",
    "We can use the `compute_uniqueness` method from the FiftyOne Brain, which measures how dissimilar each sample is from its neighbors in an embedding space. It finds each sample's 3 nearest neighbors, weights their distances (60%-30%-10%), and normalizes these weighted distances to produce scores between 0-1. Higher scores indicate samples that are more \"isolated\" or distinct in the embedding space, while lower scores indicate samples that have many close neighbors. \n",
    "\n",
    "I'll compute these values for the CLIP embeddings on the Illusion Animals subset, and leave computing uniqueness values for the other embeddings to the reader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.brain as fob\n",
    "\n",
    "fob.compute_uniqueness(\n",
    "    main_images,\n",
    "    embeddings=\"clip_embeddings\",\n",
    "    uniqueness_field=\"clip_uniqueness\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can then filter on the uniquess values to see the most and least unique images in the dataset:\n",
    "\n",
    "<img src=\"assets/illusion-animals-uniqueness.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reproducing CLIP results from the paper\n",
    "\n",
    "We've already instantiated the CLIP model above. To use it for zero shot classification, we can make use the of the `apply_model` method of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_images.apply_model(\n",
    "    model=clip_model, \n",
    "    label_field=\"clip_predictions\",\n",
    "    text_prompt = \"illusion animal \",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_res_illusions = main_images.evaluate_classifications(\n",
    "    pred_field=\"clip_predictions\",\n",
    "    gt_field=\"label\",\n",
    "    method=\"simple\",\n",
    "    eval_key=\"clip_eval\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_images.apply_model(\n",
    "    model=clip_model, \n",
    "    label_field=\"clip_predictions\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_res_filtered = filtered_images.evaluate_classifications(\n",
    "    pred_field=\"clip_predictions\",\n",
    "    gt_field=\"label\",\n",
    "    method=\"simple\",\n",
    "    eval_key=f\"clip_eval\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the Model Evaluation panel in the FiftyOne app to see the model peformance:\n",
    "\n",
    "<img src=\"assets/illusion-eval.gif\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can also print the classification reports programmatically as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_res_illusions.print_metrics(average='weighted', digits=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_res_filtered.print_metrics(average='weighted', digits=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper reported an accuracy of 42.64 for the illusion images and 85.45 for the non-illusion images.\n",
    "\n",
    "We're not able to reproduce their results, and I suspect that's because of how they implement their [inference function](https://github.com/IllusoryVQA/IllusoryVQA/blob/main/Experiments/Zero-Shot/CLIP/inference_CLIP_IllusionAnimals.ipynb):\n",
    "\n",
    "```python\n",
    "def inference(img, labels, model, vis_processors, device):\n",
    "    image = vis_processors[\"eval\"](img).unsqueeze(0).to(device)\n",
    "    sample = {\"image\": image, \"text_input\": labels}\n",
    "    clip_features = model.extract_features(sample)\n",
    "    image_features = clip_features.image_embeds_proj\n",
    "    text_features = clip_features.text_embeds_proj\n",
    "    sims = (image_features @ text_features.t())[0] / 0.01\n",
    "    probs = torch.nn.Softmax(dim=0)(sims).tolist()\n",
    "    max_index = probs.index(max(probs))\n",
    "    max_label = labels[max_index]\n",
    "    return max_label\n",
    "```\n",
    "\n",
    "1. **Feature Extraction vs End-to-End**\n",
    "\n",
    "- Paper implementation explicitly extracts features using `model.extract_features()` and then computes similarities manually\n",
    "\n",
    "- Standard implementation uses the model's built-in forward pass (`model(**inputs)`) which handles this internally\n",
    "\n",
    "2. **Temperature Scaling**\n",
    "\n",
    "- Paper implementation uses a custom temperature value of 0.01: `sims = (image_features @ text_features.t())[0] / 0.01`\n",
    "\n",
    "- Standard implementation uses CLIP's default temperature scaling built into the model\n",
    "\n",
    "\n",
    "3. **Feature Projection**\n",
    "\n",
    "- Paper specifically uses projected embeddings: `image_features = clip_features.image_embeds_proj`\n",
    "\n",
    "- Standard implementation lets the model handle the projection internally\n",
    "\n",
    "\n",
    "These differences, particularly the custom temperature value of 0.01, likely explain why we couldn't exactly reproduce their results. The temperature parameter significantly affects how \"sharp\" or \"soft\" the probability distribution becomes after softmax - a lower value like 0.01 makes the model more confident in its predictions compared to CLIP's default temperature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing SigLIP 2 and AIMv2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the AIMv2 model for zero shot classification directly with FiftyOne's integration with Hugging Face (it's just the embeddings that we needed a plugin for). Let's go ahead and instantiate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "aim_model = foz.load_zoo_model(\n",
    "    \"zero-shot-classification-transformer-torch\",\n",
    "    name_or_path=\"apple/aimv2-large-patch14-224-lit\", \n",
    "    classes=class_names,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    # install_requirements=True #yes\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And apply it to our datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_images.apply_model(\n",
    "    model=aim_model, \n",
    "    label_field=\"aimv2_predictions\",\n",
    "    text_prompt = \"illusion animal \",\n",
    "    )\n",
    "\n",
    "aim_res_illusions = main_images.evaluate_classifications(\n",
    "    pred_field=\"aimv2_predictions\",\n",
    "    gt_field=\"label\",\n",
    "    method=\"simple\",\n",
    "    eval_key=f\"aim_eval\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the filtered images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_images.apply_model(\n",
    "    model=aim_model, \n",
    "    label_field=\"aimv2_predictions\",\n",
    "    )\n",
    "\n",
    "aim_res_filtered = filtered_images.evaluate_classifications(\n",
    "    pred_field=\"aimv2_predictions\",\n",
    "    gt_field=\"label\",\n",
    "    method=\"simple\",\n",
    "    eval_key=f\"aim_eval\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's apply our already instantiated SigLIP 2 model to our datasets as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_images.apply_model(\n",
    "    model=siglip_model, \n",
    "    label_field=\"siglip2_predictions\",\n",
    "    text_prompt = \"illusion animal \",\n",
    "    )\n",
    "\n",
    "siglip_res_illusions = main_images.evaluate_classifications(\n",
    "    pred_field=\"siglip2_predictions\",\n",
    "    gt_field=\"label\",\n",
    "    method=\"simple\",\n",
    "    eval_key=f\"siglip2_eval\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for the filtered images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_images.apply_model(\n",
    "    model=siglip_model, \n",
    "    label_field=\"siglip2_predictions\",\n",
    "    )\n",
    "\n",
    "siglip_res_filtered = filtered_images.evaluate_classifications(\n",
    "    pred_field=\"siglip2_predictions\",\n",
    "    gt_field=\"label\",\n",
    "    method=\"simple\",\n",
    "    eval_key=f\"siglip2_eval\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run a comparison of model performance in the FiftyOne App as well:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary of findings\n",
    "\n",
    "| Model | Dataset Version | Your Results | Paper Results |\n",
    "|-------|----------------|--------------|---------------|\n",
    "| CLIP | Illusion Images | 56.40% | 42.64% |\n",
    "| CLIP | Filtered Images | 62.80% | 85.45% |\n",
    "| SigLIP 2 | Illusion Images | 10.40% | N/A |\n",
    "| SigLIP 2 | Filtered Images | 19.80% | N/A |\n",
    "| AIMv2 | Illusion Images | 22.9%| N/A |\n",
    "| AIMv2 | Filtered Images | 47% | N/A |\n",
    "\n",
    "I encourage you to dig into the results yourself, and if you find anything interesting please comment below. \n",
    "\n",
    "Given this isn't a research paper, and we've already covered a lot of ground I'll just share my high level observation: CLIP is crushing it! I had high hopes for the SigLIP 2 model, but on this particular task it doesn't perform as well as CLIP or AIMv2. To be fair, [I'm a huge AIMv2 fanboy](https://medium.com/voxel51/visual-understanding-with-aimv2-76c58dcd68f9)...so I was hoping it would beat both models, but it let me down here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "main_images = fo.load_dataset(\"illusion_animals\")\n",
    "\n",
    "filtered_images= fo.load_dataset(\"illusion_animals_fitered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download https://github.com/harpreetsahota204/janus-vqa-fiftyone\n",
    "\n",
    "!fiftyone plugins requirements @harpreetsahota/janus_vqa --install\n",
    "\n",
    "!fiftyone plugins download https://github.com/harpreetsahota204/moondream2-plugin\n",
    "\n",
    "!fiftyone plugins requirements @harpreetsahota/moondream2 --install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_HINT_PROMPT = f\"\"\"Which class is in the picture: {', '.join(class_names)}. \n",
    "Your answer must be one of these exact classes, no other answers allowed. \n",
    "Respond in one word for your guess of the correct class without any extra explanation.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.operators as foo\n",
    "\n",
    "janus_vqa = foo.get_operator(\"@harpreetsahota/janus_vqa/janus_vqa\")\n",
    "\n",
    "moondream = foo.get_operator(\"@harpreetsahota/moondream2/moondream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No hint prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "await janus_vqa(\n",
    "    dataset,\n",
    "    model_path=\"deepseek-ai/Janus-Pro-1B\",\n",
    "    question=NO_HINT_PROMPT,\n",
    "    question_field=\"no_hint_prompt\",\n",
    "    answer_field=\"janus_no_hint_answer\",\n",
    "    delegate=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await moondream(\n",
    "    dataset,\n",
    "    revision=\"2025-01-09\",\n",
    "    operation=\"query\",\n",
    "    output_field=\"moondream_no_hint_answer\",\n",
    "    query_text=NO_HINT_PROMPT,\n",
    "    delegate=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "HINT_PROMPT = f\"\"\"There might be an image illusion of something in this image. \n",
    "These are the classes that the image illusion might belong to: {', '.join(class_names)}.\n",
    "Your answer must be one of these exact classes, no other answers allowed.  \n",
    "Respond in one word for your guess of the correct class without any extra explanation.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "await janus_vqa(\n",
    "    dataset,\n",
    "    model_path=\"deepseek-ai/Janus-Pro-1B\",\n",
    "    question=HINT_PROMPT,\n",
    "    question_field=\"hint_prompt\",\n",
    "    answer_field=\"janus_hint_answer\",\n",
    "    delegate=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await moondream(\n",
    "    dataset,\n",
    "    revision=\"2025-01-09\",\n",
    "    operation=\"query\",\n",
    "    output_field=\"moondream_hint_answer\",\n",
    "    query_text=HINT_PROMPT,\n",
    "    delegate=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moondream2 also produces short captions, let's generate short captions and then compute similarity between the caption and the ground truth prompt\n",
    "\n",
    "Then let's also see if any of the captions actually include the classes of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why This Matters for AI Practitioners and Researchers\n",
    "\n",
    "This research opens exciting new avenues for improving multimodal AI systems:\n",
    "\n",
    "1. **Perceptual Robustness**: Training models to handle illusory images could make them more robust to adversarial attacks and unusual visual inputs. If a model can correctly process information even when presented with potentially misleading visual cues, it may be less susceptible to manipulation or confusion in real-world applications.\n",
    "\n",
    "2. **Cognitive Alignment**: Understanding how AI models perceive illusions differently from humans can help researchers better align AI visual processing with human cognition. This alignment is crucial for applications where AI systems need to interpret visual information similarly to humans, such as in autonomous driving or medical image analysis.\n",
    "\n",
    "3. **Preprocessing Solutions**: The simple filtering technique offers an immediate way to improve model performance on challenging visual inputs without requiring extensive retraining or architectural changes.\n",
    "\n",
    "4. **Benchmark Advancement**: The Illusory VQA datasets provide valuable new benchmarks that push beyond conventional image recognition tasks, helping researchers identify strengths and weaknesses in current multimodal architectures.\n",
    "\n",
    "5. **Bridging Disciplines**: This work creates interesting connections between AI research and cognitive psychology, potentially leading to cross-disciplinary insights about visual perception.\n",
    "\n",
    "For AI practitioners working on multimodal systems, incorporating illusion testing into evaluation protocols could reveal important limitations that might otherwise go undetected. Similarly, the preprocessing techniques described in this research could be adapted for a variety of challenging visual inputs beyond just illusions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "fiftyone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
