{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When humans encounter optical illusions, our brains often see things that aren't physically present in the image. \n",
    "\n",
    "This perceptual phenomenon, known as pareidolia, has long fascinated neuroscientists and psychologists. Now, researchers are turning these visual puzzles toward Vision-Language Models (VLM) to test their perceptual capabilities.\n",
    "\n",
    "I recently came across a paper,*[Illusory VQA: Benchmarking and Enhancing Multimodal Models on Visual Illusions\n",
    "](https://arxiv.org/abs/2412.08169)*, which introduces a novel task called Illusory VQA. The core challenge presented in the [Illusory VQA](https://github.com/IllusoryVQA/IllusoryVQA) task is deceptively complex: given an image containing both a \"Real Concept\" (RC) and potentially an \"Illusory Concept\" (IC), can a VLM detect if an illusion is present and correctly answer questions about that illusory element?\n",
    "\n",
    "This task requires perception beyond standard image recognition and assessing how well models can mimic human-like visual understanding, and is interestingly challenging because the model must simultaneously recognize what's actually in the image while also perceiving what appears to be there due to the illusion — much like our own visual system does.\n",
    "\n",
    "### The Illusory Datasets\n",
    "\n",
    "For this task, the authors created four benchmark datasets, each targeting different aspects of visual illusion processing:\n",
    "\n",
    "• **IllusionMNIST:** Built using the classic MNIST handwritten digit dataset as the source material, this dataset contains 3,960 training samples and 1,219 test samples. The researchers added a \"No illusion\" class to make the task more challenging, requiring models to determine whether an illusion is actually present.\n",
    "\n",
    "• **IllusionFashionMNIST:** Based on the Fashion-MNIST dataset, which contains clothing items rather than digits, this collection includes 3,300 training samples and 1,267 test samples. Like its MNIST counterpart, it includes a \"No illusion\" class to further test discrimination abilities.\n",
    "\n",
    "• **IllusionAnimals:** This dataset features animal images generated using SDXL-Lightning and transformed with ControlNet to create illusory versions. It comprises 3,300 training samples and 1,100 test samples, with the additional \"No illusion\" class.\n",
    "\n",
    "• **IllusionChar:** This unique dataset focuses specifically on reading characters in images, with sequences of 3 to 5 characters per image. Including 9,900 training samples and 3,300 test samples, it tests how well models can interpret text within illusory contexts.\n",
    "\n",
    "What I found particularly interesting is how these datasets were constructed:\n",
    "\n",
    "<img src=\"assets/illusoryvqa-datagen.png\" width=\"70%\">\n",
    "\n",
    "The research team:\n",
    "\n",
    "- Generated scene descriptions using large language models\n",
    "\n",
    "- Combined these descriptions with raw images\n",
    "\n",
    "- Used a variant of ControlNet to create the final illusory images\n",
    "\n",
    "- Conducted human evaluations to validate the quality of the generated images\n",
    "\n",
    "- Asked participants to identify what they perceived in each picture\n",
    "\n",
    "- Filtered out inappropriate content using NSFW detectors\n",
    "\n",
    "This approach ensures that the illusions in the datasets genuinely challenge perceptual abilities in ways that mirror human visual processing.\n",
    "\n",
    "### Testing Leading Multimodal Models\n",
    "\n",
    "The study evaluated several state-of-the-art models:\n",
    "\n",
    "<img src=\"assets/illusoryvqa-table2.png\" width=\"70%\">\n",
    "\n",
    "The research team focused on zero-shot performance (how well models perform without specific training on illusions) and performance after fine-tuning.\n",
    "\n",
    "The results that all models showed a performance drop when dealing with illusions compared to standard images—mirroring the human experience of being \"fooled\" by optical illusions. Different models demonstrated varying levels of robustness to different types of illusions, suggesting that architectural differences influence how these systems process visual information.\n",
    "\n",
    "### A Simple Yet Effective Solution\n",
    "\n",
    "An interesting finding from the research is their straightforward solution for improving model performance on illusory images. The technique:\n",
    "\n",
    "1. Apply a Gaussian and blur low-pass filter to the illusory images\n",
    "2. Convert the images to grayscale\n",
    "\n",
    "This simple preprocessing approach yielded significant performance improvements across all tested models. \n",
    "\n",
    "For example, in the IllusionAnimals dataset:\n",
    "\n",
    "- CLIP initially showed the highest performance on illusory images\n",
    "\n",
    "- After applying the filter, BLIP-2 achieved the best results—even outperforming humans\n",
    "\n",
    "- All models saw substantial gains in accuracy after implementing the filtering technique\n",
    "\n",
    "This finding suggests that relatively simple image processing techniques can help AI systems overcome perceptual challenges posed by illusions. The filtering process essentially helps the models differentiate between real and illusory elements in the images, similar to how certain visual processing aids might help humans see through optical illusions.\n",
    "\n",
    "## What we're going to do in this tutorial\n",
    "\n",
    "\n",
    "In this tutorial, we'll explore the IllusionAnimals dataset and evaluate how different AI models perceive visual illusions. We'll:\n",
    "\n",
    "1. Load and explore the IllusionAnimals dataset using FiftyOne and see if we can reproduce the results from the paper, but only focusing on the CLIP model.\n",
    "\n",
    "2. Compute embeddings using multiple models:\n",
    "   - CLIP  \n",
    "   - SigLIP 2 (a new model released by Google)\n",
    "   - AIMv2 (in my opinion a highly slept on contender to CLIP released in late 2024 by Apple)\n",
    "\n",
    "3. Visualize these embeddings using UMAP dimensionality reduction\n",
    "\n",
    "4. Perform zero-shot classification using the models mentioned above\n",
    "\n",
    "5. Test Visual Question-Answering (VQA) capabilities using:\n",
    "   - Janus-Pro\n",
    "   - Moondream2\n",
    "\n",
    "6. Compare how models perform with and without hints about potential illusions\n",
    "\n",
    "Let's start by installing some dependencies and downloading the dataset from the Hugging Face Hub "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing bleeding edge version of transformers\n",
    "!pip install git+https://github.com/huggingface/transformers.git#egg=transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fiftyone umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "from fiftyone.utils.huggingface import load_from_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_hub(\n",
    "    \"harpreetsahota/IllusionAnimals\",\n",
    "    overwrite=True,\n",
    "    persistent=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with an initial visual vibe check of what is in this dataset.\n",
    "\n",
    "Note this is a [Grouped Dataset](https://docs.voxel51.com/user_guide/groups.html#grouped-datasets). Grouped datasets allow us to represent multiples slices of the same data point. This way data for multiple perspectives of the same scene can be stored, visualized, and queried in ways that respect the relationships between the slices of data.\n",
    "\n",
    "For the IllusionAnimals subset, the dataset includes different slices representing variations of the images with and without illusions, and with and without filters. The specific slices available in the IllusionAnimals dataset are:\n",
    "\n",
    "*   **Raw Images**: These are the original images of animals without any illusions applied. They serve as a baseline for evaluating the models' performance on standard image recognition tasks. The models should accurately identify the animal in the image.\n",
    "\n",
    "*   **Illusory Images**: These images have visual illusions incorporated into them. The illusions are designed to make the images appear as one animal while subtly containing elements of another. The goal is to test whether the models can detect the presence of the illusory concept, even with the presence of the real concept.\n",
    "\n",
    "*   **Filtered Images**: These are the illusory images that have been processed with a Gaussian and blur low-pass filter. This filter is applied to enhance the models’ ability to detect the illusions. The idea is that the filter helps to reduce noise and highlight the illusory elements, making it easier for the models to identify and interpret the content. Applying the filter generally improves model performance.\n",
    "\n",
    "*   **Illusionless Class**: In addition to the above, an extra class called \"illusionless\" is added to push the models’ capabilities. This class enables the models to detect instances where no illusion images are present in the picture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need the classes for zero-shot classification later, let's get them now. Note that when I originally parsed the dataset I mapped the images with no illusion to the `illusionless` class. To be consisent with the paper, I'm going to map these to the `no illusion` class. I was too lazy to reparse the dataset, but luckily this is easy to do in FiftyOne using the `map_labels` method of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map_labels(\"label\", {\"illusionless\": \"no illusion\"}) # rename label \"illusionless\" to \"no illusion\"\n",
    "\n",
    "dataset.save() # save the changes\n",
    "\n",
    "class_names = dataset.distinct(\"label.label\") # get the class names "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're only going to work with two of the slices in this tutorial, \"main\" (the illusion and no illusion images) and \"filtered\" (which are the images after the Gaussian Blur and grayscaling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_images = dataset.select_group_slices(\"main\")\n",
    "\n",
    "filtered_images = dataset.select_group_slices(\"filtered\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Embeddings for Deeper Dataset Understanding\n",
    "\n",
    "The first thing I want to do is gain a deeper understanding of the images in this dataset, for that we can use embeddings.\n",
    "\n",
    "Visual embeddings are high-dimensional vector representations of images that capture semantic and visual features. For the IllusionAnimals dataset, embeddings are particularly valuable because they can help us:\n",
    "\n",
    "1. **Visualize Relationships**: Reducing these high-dimensional embeddings to 2D using UMAP helps visualize how different images cluster together and potentially identify patterns in the dataset.\n",
    "\n",
    "2. **Compare Model Perspectives**: Different models may encode visual information differently. By comparing embeddings from multiple models (SigLIP and AIM-v2 in our case), we can understand how their \"perception\" of illusions differs.\n",
    "\n",
    "3. **Analyze Illusion Effects**: We can examine whether illusory versions of images cluster closer to their \"real\" concept or their \"illusory\" concept, giving us insights into how effectively the illusions work from a model's perspective.\n",
    "\n",
    "For this analysis, we'll use three models:\n",
    "\n",
    "- **CLIP**\n",
    "\n",
    "- **SigLIP 2** \n",
    "\n",
    "- **AIM-v2** \n",
    "\n",
    "Let's start by instantiating the models and then computing embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CLIP\n",
    "\n",
    "There's been a lot written about the CLIP model, so I won't repeat anything here. However, if you're interested in going deep into CLIP and its history then [check out this blog](https://voxel51.com/blog/a-history-of-clip-model-training-data-advances/).\n",
    "\n",
    "We can use the model via [FiftyOne's integration with Hugging Face](https://docs.voxel51.com/integrations/huggingface.html#zero-shot-classification). Note that we will instantiate the model with classes but it will not affect the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "clip_model = foz.load_zoo_model(\n",
    "    \"zero-shot-classification-transformer-torch\",\n",
    "    name_or_path=\"openai/clip-vit-base-patch32\", \n",
    "    classes=class_names,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    install_requirements=True\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_images.compute_embeddings(\n",
    "    model=clip_model,\n",
    "    embeddings_field=\"clip_embeddings\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_images.compute_embeddings(\n",
    "    model=clip_model,\n",
    "    embeddings_field=\"clip_embeddings\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SigLIP 2\n",
    "\n",
    "SigLIP 2 is a [family of multilingual vision-language encoders](https://huggingface.co/collections/google/siglip2-67b5dcef38c175486e240107) that improves upon the original SigLIP model. It incorporates several techniques, including captioning-based pretraining, self-supervised losses (self-distillation, masked prediction), and online data curation, into a unified training recipe.\n",
    "\n",
    "I don't want to get too deep into the technical details of the SigLIP 2 model, feel free to [check out the paper](https://arxiv.org/abs/2502.14786) for more information about the model, it's performance, and how it was trained.\n",
    "\n",
    "##### SigLIP 2 Data Curation\n",
    "\n",
    "What I do want to spend some time talking about, however, is the data curation methods discussed in the paper. The data curation method in focuses on improving the quality and diversity of training data, especially for smaller models. The primary technique used is **[Active Curation as Implicit Distillation (ACID)](https://arxiv.org/pdf/2411.18674)**. Here's a breakdown of how it works:\n",
    "\n",
    "1.  **Teacher-Student Model Setup**:\n",
    "    *   A smaller model (student) is trained with the help of a more powerful, pre-trained model (teacher).\n",
    "\n",
    "    *   In SigLIP 2, the **teacher model is a fine-tuned SigLIP 2 So400m model**, which is initially trained on a diverse dataset and then further fine-tuned on a high-quality curated dataset.\n",
    "\n",
    "2.  **Scoring Examples for \"Learnability\"**:\n",
    "\n",
    "    *   During training, both the teacher and student models evaluate the training examples.\n",
    "\n",
    "    *   Each example is scored based on how \"learnable\" it is. This score is basically the difference in loss values between the current student model and the reference model, and is meant to reflect how well the model can learn from that particular example.\n",
    "\n",
    "     *  Examples that are easy for the reference but difficult for the current student are given high scores. This means the data is \"learnable\" because the reference model, which has already been trained, can easily understand the patterns in the data. The student model, still in training, finds these patterns more challenging.\n",
    "\n",
    "3.  **Active Sample Selection**:\n",
    "\n",
    "    *   Instead of using all available data, **only the most \"learnable\" examples are selected** for each training batch.\n",
    "\n",
    "    *   This selection is done jointly, with two main criteria for scoring sub-batches:\n",
    "\n",
    "        *   Easy-reference scoring: This uses the loss values of the reference model to preferentially sample batches that are easy for the reference model.\n",
    "\n",
    "        *   Learnability scoring: This uses the difference in loss values between the current student model and the reference model to prioritize batches that are easy for the reference model but difficult for the student model. Batches that are easy for the reference but difficult for the current student are given high scores\n",
    "\n",
    "    *   By curating data based on the reference model, ACID implicitly distills its knowledge through a data-driven objective. This objective combines model predictions and real labels as targets, and retains targets where the reference model and labels agree allowing for mutual denoising of model predictions and data labels\n",
    "\n",
    "4.  **Filtering Ratio**:\n",
    "    *   To balance the benefits of curation with computational costs, a filtering ratio is applied.\n",
    "\n",
    "    *   For example, a filtering ratio of 0.5 means that the super-batch is twice the size of the final batch (64,000 examples), and the best 50% are selected.\n",
    "\n",
    "    *   The B/32 model uses a filtering ratio of 0.75.\n",
    "\n",
    "5.  **Implicit Distillation**:\n",
    "\n",
    "    *   By selectively training on the most informative examples, the student model (the smaller SigLIP 2 model) learns to mimic the behavior of the teacher model.\n",
    "\n",
    "    *   This process implicitly distills the knowledge from the larger, more capable teacher model into the smaller student model, improving its performance.\n",
    "\n",
    "    *   This method saves computational resources compared to explicit distillation methods that may use a second teacher model.\n",
    "\n",
    "In essence, this data curation method ensures that the smaller models are trained on the most valuable data, leading to improved performance and efficiency. By fine-tuning the teacher model on a curated dataset, the method also captures the benefits of diverse knowledge and high-quality data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "siglip_model = foz.load_zoo_model(\n",
    "    \"zero-shot-classification-transformer-torch\",\n",
    "    name_or_path=\"google/siglip2-base-patch32-256\", \n",
    "    classes=class_names,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_images.compute_embeddings(\n",
    "    model=siglip_model,\n",
    "    embeddings_field=\"siglip_embeddings\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_images.compute_embeddings(\n",
    "    model=siglip_model,\n",
    "    embeddings_field=\"siglip_embeddings\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AIMv2\n",
    "\n",
    "AIMv2 is a [family of vision encoders released in late 2024](huggingface.co/collections/apple/aimv2-6720fe1558d94c7805f7688c) that uses a novel multimodal autoregressive method. \n",
    "\n",
    "It processes image patches and text tokens as a unified sequence, using a causal multimodal decoder to predict elements sequentially. AIMv2 processes data as one continuous sequence, predicting the next step in the series. It deliberately puts image information first, followed by text, creating a specific sequence: image patches → text tokens. This differs from CLIP's parallel processing of image and text and strengthens the vision encoder. AIMv2 is trained on 12 billion image-text samples, balancing human-written alt-text and synthetically generated captions from diverse sources. I've written about the AIMv2 models in great detail in two blog posts, which you can read [here](https://medium.com/voxel51/visual-understanding-with-aimv2-76c58dcd68f9) and [here](https://medium.com/voxel51/aimv2-outperforms-clip-on-synthetic-dataset-imagenet-d-4452760b624c).\n",
    "\n",
    "In order to use AIMv2 for embeddings, we need to [install a plugin](https://github.com/harpreetsahota204/aim-embeddings-plugin)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download https://github.com/harpreetsahota204/aimv2_embeddings\n",
    "\n",
    "!fiftyone plugins requirements @harpreetsahota/aimv2_embeddings --install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to set an enviornment variable as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['FIFTYONE_ALLOW_LEGACY_ORCHESTRATORS'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.operators as foo\n",
    "\n",
    "aim_embeddings = foo.get_operator(\"@harpreetsahota/aimv2_embeddings/compute_aimv2_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the operator on your dataset\n",
    "await aim_embeddings(\n",
    "    main_images,\n",
    "    model_name=\"apple/aimv2-large-patch14-224-lit\",  # Choose any supported model\n",
    "    embedding_types=\"cls\", #can be \"cls\", \"mean\"\n",
    "    emb_field=\"aimv2_embeddings\",\n",
    "    delegate=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the operator on your dataset\n",
    "await aim_embeddings(\n",
    "    filtered_images,\n",
    "    model_name=\"apple/aimv2-large-patch14-224-lit\",  # Choose any supported model\n",
    "    embedding_types=\"cls\",\n",
    "    emb_field=\"aimv2_embeddings\",\n",
    "    delegate=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use UMAP to reduce the dimensionality of the embeddings "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.brain as fob\n",
    "\n",
    "# Define datasets and embedding fields\n",
    "datasets = {\n",
    "    \"main\": main_images,\n",
    "    \"filtered\": filtered_images\n",
    "}\n",
    "\n",
    "embedding_fields = [ \n",
    "    \"aimv2_embeddings\",  # Updated to match the actual field name\n",
    "    \"clip_embeddings\",\n",
    "    \"siglip_embeddings\"\n",
    "]\n",
    "\n",
    "# Compute UMAP for each dataset and embedding combination\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    for field in embedding_fields:\n",
    "        _fname = field.split(\"_embeddings\")[0]\n",
    "        brain_key = f\"{_fname}_{dataset_name}_viz\"\n",
    "        \n",
    "        results = fob.compute_visualization(\n",
    "            dataset,\n",
    "            embeddings=field,\n",
    "            method=\"umap\",\n",
    "            brain_key=brain_key,\n",
    "            num_dims=2,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reproducing CLIP results from the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing SigLIP 2 and AIMv2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the AIMv2 model for zero shot classification directly with FiftyOne's integration with Hugging Face (it's just the embeddings that we needed a plugin for). Let's go ahead and instantiate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "aim_model = foz.load_zoo_model(\n",
    "    \"zero-shot-classification-transformer-torch\",\n",
    "    name_or_path=\"apple/aimv2-large-patch14-224-lit\", \n",
    "    classes=class_names,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    install_requirements=True\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.apply_model(\n",
    "    model=aim_model, \n",
    "    label_field=\"aimv2_predictions\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.apply_model(\n",
    "    model=siglip_model, \n",
    "    label_field=\"siglip2_predictions\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate classifications and see the results\n",
    "\n",
    "\n",
    "### Can VLMs do any better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download https://github.com/harpreetsahota204/janus-vqa-fiftyone\n",
    "\n",
    "!fiftyone plugins requirements @harpreetsahota/janus_vqa --install\n",
    "\n",
    "!fiftyone plugins download https://github.com/harpreetsahota204/moondream2-plugin\n",
    "\n",
    "!fiftyone plugins requirements @harpreetsahota/moondream2 --install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NO_HINT_PROMPT = f\"\"\"Which class is in the picture: {', '.join(class_names)}. \n",
    "Your answer must be one of these exact classes, no other answers allowed. \n",
    "Respond in one word for your guess of the correct class without any extra explanation.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.operators as foo\n",
    "\n",
    "janus_vqa = foo.get_operator(\"@harpreetsahota/janus_vqa/janus_vqa\")\n",
    "\n",
    "moondream = foo.get_operator(\"@harpreetsahota/moondream2/moondream\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No hint prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "await janus_vqa(\n",
    "    dataset,\n",
    "    model_path=\"deepseek-ai/Janus-Pro-1B\",\n",
    "    question=NO_HINT_PROMPT,\n",
    "    question_field=\"no_hint_prompt\",\n",
    "    answer_field=\"janus_no_hint_answer\",\n",
    "    delegate=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await moondream(\n",
    "    dataset,\n",
    "    revision=\"2025-01-09\",\n",
    "    operation=\"query\",\n",
    "    output_field=\"moondream_no_hint_answer\",\n",
    "    query_text=NO_HINT_PROMPT,\n",
    "    delegate=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "HINT_PROMPT = f\"\"\"There might be an image illusion of something in this image. \n",
    "These are the classes that the image illusion might belong to: {', '.join(class_names)}.\n",
    "Your answer must be one of these exact classes, no other answers allowed.  \n",
    "Respond in one word for your guess of the correct class without any extra explanation.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "await janus_vqa(\n",
    "    dataset,\n",
    "    model_path=\"deepseek-ai/Janus-Pro-1B\",\n",
    "    question=HINT_PROMPT,\n",
    "    question_field=\"hint_prompt\",\n",
    "    answer_field=\"janus_hint_answer\",\n",
    "    delegate=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await moondream(\n",
    "    dataset,\n",
    "    revision=\"2025-01-09\",\n",
    "    operation=\"query\",\n",
    "    output_field=\"moondream_hint_answer\",\n",
    "    query_text=HINT_PROMPT,\n",
    "    delegate=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moondream2 also produces short captions, let's generate short captions and then compute similarity between the caption and the ground truth prompt\n",
    "\n",
    "Then let's also see if any of the captions actually include the classes of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why This Matters for AI Practitioners and Researchers\n",
    "\n",
    "This research opens exciting new avenues for improving multimodal AI systems:\n",
    "\n",
    "1. **Perceptual Robustness**: Training models to handle illusory images could make them more robust to adversarial attacks and unusual visual inputs. If a model can correctly process information even when presented with potentially misleading visual cues, it may be less susceptible to manipulation or confusion in real-world applications.\n",
    "\n",
    "2. **Cognitive Alignment**: Understanding how AI models perceive illusions differently from humans can help researchers better align AI visual processing with human cognition. This alignment is crucial for applications where AI systems need to interpret visual information similarly to humans, such as in autonomous driving or medical image analysis.\n",
    "\n",
    "3. **Preprocessing Solutions**: The simple filtering technique offers an immediate way to improve model performance on challenging visual inputs without requiring extensive retraining or architectural changes.\n",
    "\n",
    "4. **Benchmark Advancement**: The Illusory VQA datasets provide valuable new benchmarks that push beyond conventional image recognition tasks, helping researchers identify strengths and weaknesses in current multimodal architectures.\n",
    "\n",
    "5. **Bridging Disciplines**: This work creates interesting connections between AI research and cognitive psychology, potentially leading to cross-disciplinary insights about visual perception.\n",
    "\n",
    "For AI practitioners working on multimodal systems, incorporating illusion testing into evaluation protocols could reveal important limitations that might otherwise go undetected. Similarly, the preprocessing techniques described in this research could be adapted for a variety of challenging visual inputs beyond just illusions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "fiftyone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
