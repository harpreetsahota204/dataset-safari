{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIMv2 vs CLIP Robustness on ImageNet-D\n",
    "\n",
    "![ImageNet-D Examples](assets/imagenet-d.gif)\n",
    "\n",
    "\n",
    "ImageNet-D is a new benchmark of synthetically generated images (via Stable Diffusion) that's pushing image classification models to their breaking points with challenging images and revealing critical failures in model robustness. \n",
    "\n",
    "A high-level overview of ImageNet-D:\n",
    "\n",
    "* It's composed of 4,835 \"hard images.\" \n",
    "\n",
    "* ImageNet-D spans 113 overlapping categories between ImageNet and ObjectNet.\n",
    "\n",
    "* The dataset incorporates 547 nuisance variations, including a wide array of backgrounds (3,764), textures (498), and materials (573), making it far more diverse than previous benchmarks. By systematically varying these factors, ImageNet-D comprehensively assesses how well a model can truly \"see\" beyond superficial image features.\n",
    "\n",
    "At the heart of ImageNet-D is the concept of \"hard images\". To create a challenging test, the researchers employed a clever strategy to mine hard samples:\n",
    "\n",
    "* They generated a large pool of images using diffusion models.\n",
    "\n",
    "* They then used a set of \"surrogate models\" (pre-trained vision models) to identify images that were commonly misclassified.\n",
    "\n",
    "* Only these challenging \"hard images\" were retained for the final ImageNet-D dataset. This ensures that the benchmark focuses on the weaknesses of current models and provides a more informative evaluation.\n",
    "\n",
    "I wrote an in-depth blog about this dataset, which you can read [here](https://medium.com/voxel51/imagenet-d-a-new-synthetic-test-set-designed-to-rigorously-evaluate-the-robustness-of-neural-ab8978716585).\n",
    "\n",
    "### What we're doing in this tutorial.\n",
    "\n",
    "In this tutorial, you're going to:\n",
    "\n",
    "1. Explore the ImageNet-D dataset using FiftyOne\n",
    "\n",
    "2. Compute and visualize the embeddings for the images in this dataset using AIMv2 and CLIP to gain a deeper understanding of it's contents\n",
    "\n",
    "3. Perfom zero-shot classification using CLIP in an attempt to verify/replicate the results in the paper\n",
    "\n",
    "4. Perform zero-shot classification using AIMv2\n",
    "\n",
    "5. Compare each models performance to the ground truth labels to see which performs better\n",
    "\n",
    "# Preliminaries\n",
    "\n",
    "Let's kick things off by installing FiftyOne, some dependencies needed for this tutorial, and then downloading the ImageNet-D dataset from the [Voxel51 org on Hugging Face](https://huggingface.co/Voxel51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fiftyone umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import fiftyone.utils.huggingface as fouh\n",
    "\n",
    "dataset = fouh.load_from_hub(\n",
    "    \"Voxel51/ImageNet-D\",\n",
    "    name=\"imagenet_d\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the dataset has been downloaded, you can do some initial exploration by launching the app:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_labels = dataset.distinct(\"ground_truth.label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download https://github.com/harpreetsahota204/aim-embeddings-plugin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**How AIMv2 Differs from CLIP**  \n",
    "\n",
    "**Core Training Differences**  \n",
    "| **AIMv2** | **CLIP** |  \n",
    "|-----------|----------|  \n",
    "| Uses **autoregressive modeling** to reconstruct inputs *sequentially* (image patches → text tokens) | Uses **contrastive learning** to align *parallel* image-text pairs |  \n",
    "| Processes images and text as a **unified sequence** | Processes modalities **separately** |  \n",
    "| Extracts training signals from **every token** (dense supervision) | Relies on **positive/negative pair contrast** (sparse supervision) |  \n",
    "| Requires **no specialized batch processing** | Demands **large batches** for effective negative sampling |  \n",
    "| Learns **implicit relationships** via sequential prediction | Forces **explicit alignment** of embeddings |  \n",
    "\n",
    "**Sequence Architecture: Why Order Matters**  \n",
    "AIMv2 deliberately processes **image patches first**, followed by text tokens:  \n",
    "1. **Visual Foundation**: Text predictions leverage *complete* visual context (like describing a photo only after seeing it in full).  \n",
    "2. **Unified Processing**: Predicts next image patches (e.g., reconstructing a photo’s bottom half from the top), then generates text autoregressively (e.g., completing \"A dog plays in...\" → \"park\").  \n",
    "3. **Vision-Centric Design**: Forces robust visual representations to support both image reconstruction *and* text generation.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.operators as foo\n",
    "\n",
    "aim_embeddings = foo.get_operator(\"@harpreetsahota/aimv2_embeddings/compute_aimv2_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_types = ['cls', 'mean']\n",
    "\n",
    "for emb_type in embedding_types:\n",
    "  await aim_embeddings(\n",
    "      dataset,\n",
    "      model_name=\"apple/aimv2-large-patch14-224\",\n",
    "      embedding_types=emb_type,\n",
    "      emb_field=f\"aimv2_{emb_type}_emb\",\n",
    "      delegate=True\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "clip_model = foz.load_zoo_model(\n",
    "    \"clip-vit-base32-torch\",\n",
    "    text_prompt=\"A photo of a\",\n",
    "    classes=gt_labels,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    )\n",
    "\n",
    "dataset.compute_embeddings(\n",
    "    model=clip_model,\n",
    "    embeddings_field=\"clip_emb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.brain as fob\n",
    "\n",
    "embedding_fields = [\"aimv2_cls_emb\", \"aimv2_mean_emb\", \"clip_emb\"]\n",
    "\n",
    "for embeddings in embedding_fields:\n",
    "  results = fob.compute_visualization(\n",
    "      dataset,\n",
    "      embeddings=embeddings,\n",
    "      method=\"umap\",\n",
    "      brain_key=f\"{embeddings}_viz\",\n",
    "      num_dims=2,\n",
    "      n_neighbors=10,\n",
    "      min_dist=0.051,\n",
    "      verbose=True,\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download https://github.com/jacobmarks/zero-shot-prediction-plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.operators as foo\n",
    "\n",
    "zsc = foo.get_operator(\"@jacobmarks/zero_shot_prediction/zero_shot_classify\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await zsc(\n",
    "    dataset,\n",
    "    labels=gt_labels,\n",
    "    model_name=\"AIMv2\",\n",
    "    label_field=\"AIMv2_predictions\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.apply_model(\n",
    "    model=clip_model, \n",
    "    label_field=\"clip_predictions\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "fiftyone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
