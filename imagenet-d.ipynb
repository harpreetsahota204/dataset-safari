{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AIMv2 vs CLIP Robustness on ImageNet-D\n",
    "\n",
    "<img src=\"/assets/imagenet-d.gif\">\n",
    "\n",
    "\n",
    "ImageNet-D is a new benchmark of synthetically generated images (via Stable Diffusion) that's pushing image classification models to their breaking points with challenging images and revealing critical failures in model robustness. \n",
    "\n",
    "A high-level overview of ImageNet-D:\n",
    "\n",
    "* It's composed of 4,835 \"hard images.\" \n",
    "\n",
    "* ImageNet-D spans 113 overlapping categories between ImageNet and ObjectNet.\n",
    "\n",
    "* The dataset incorporates 547 nuisance variations, including a wide array of backgrounds (3,764), textures (498), and materials (573), making it far more diverse than previous benchmarks. By systematically varying these factors, ImageNet-D comprehensively assesses how well a model can truly \"see\" beyond superficial image features.\n",
    "\n",
    "At the heart of ImageNet-D is the concept of \"hard images\". To create a challenging test, the researchers employed a clever strategy to mine hard samples:\n",
    "\n",
    "* They generated a large pool of images using diffusion models.\n",
    "\n",
    "* They then used a set of \"surrogate models\" (pre-trained vision models) to identify images that were commonly misclassified.\n",
    "\n",
    "* Only these challenging \"hard images\" were retained for the final ImageNet-D dataset. This ensures that the benchmark focuses on the weaknesses of current models and provides a more informative evaluation.\n",
    "\n",
    "I wrote an in-depth blog about the ImageNet-D dataset, which you can read [here](https://medium.com/voxel51/imagenet-d-a-new-synthetic-test-set-designed-to-rigorously-evaluate-the-robustness-of-neural-ab8978716585).\n",
    "\n",
    "### What we're doing in this tutorial.\n",
    "\n",
    "In this tutorial, you're going to:\n",
    "\n",
    "1. Explore the ImageNet-D dataset using FiftyOne\n",
    "\n",
    "2. Compute and visualize the embeddings for the images in this dataset using AIMv2 and CLIP to gain a deeper understanding of it's contents\n",
    "\n",
    "3. Perfom zero-shot classification using CLIP in an attempt to verify/replicate the results in the paper\n",
    "\n",
    "4. Perform zero-shot classification using AIMv2\n",
    "\n",
    "5. Compare each models performance to the ground truth labels to see which performs better\n",
    "\n",
    "# Preliminaries\n",
    "\n",
    "Let's kick things off by installing FiftyOne, some dependencies needed for this tutorial, and then downloading the ImageNet-D dataset from the [Voxel51 org on Hugging Face](https://huggingface.co/Voxel51)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fiftyone umap-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading config file fiftyone.yml from Voxel51/ImageNet-D\n",
      "Loading dataset\n",
      "Importing samples...\n",
      " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4835/4835 [68.3ms elapsed, 0s remaining, 70.8K samples/s]   \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import fiftyone as fo\n",
    "import fiftyone.utils.huggingface as fouh\n",
    "\n",
    "os.environ['FIFTYONE_ALLOW_LEGACY_ORCHESTRATORS'] = 'true'\n",
    "\n",
    "dataset = fouh.load_from_hub(\n",
    "    \"Voxel51/ImageNet-D\",\n",
    "    name=\"imagenet_d\",\n",
    "    overwrite=True,\n",
    "    persistent=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's install a plugin that allows us to create custom dashboards and glean more insight into our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download \\\n",
    "    https://github.com/voxel51/fiftyone-plugins \\\n",
    "    --plugin-names @voxel51/dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the dataset has been downloaded, you can do some initial exploration by launching the app. \n",
    "\n",
    "There are two ways to use the app:\n",
    "\n",
    "1. As a cell in your notebook, which you can do by running `fo.launch_app(dataset)`\n",
    "\n",
    "2. In a seperate browser window by running `fiftyone app launch` in your terminal\n",
    "\n",
    "Once the app is launched you can explore the dataset by:\n",
    "\n",
    "* Simply scrolling through the images for an initial \"vibe check\" for what's in it\n",
    "\n",
    "* Filtering by classes using the sidebar\n",
    "\n",
    "* Creating a dashboard with a plot for class frequence\n",
    "\n",
    "![ImageNet-D Examples](assets/explore-imagenetd-in-fo.gif)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to need the ground truth labels later, so lets go ahead and grab them from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_labels = dataset.distinct(\"ground_truth.label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is AIMv2?\n",
    "\n",
    "AIMV2 is a family of **open vision encoders** pre-trained using a novel **multimodal autoregressive objective**. \n",
    "\n",
    "It autoregressively generates both **image patches and text tokens**, leveraging signals from all input tokens and patches for efficient training. AIMV2 uses a causal multimodal decoder that first regresses image patches and then decodes text tokens in an autoregressive manner. This model excels in tasks like **image recognition, grounding, and multimodal understanding**. AIMV2 consistently matches or outperforms existing self-supervised and vision-language pre-trained models.\n",
    "\n",
    "AIMv2 deliberately processes **image patches first**, followed by text tokens:  \n",
    "\n",
    "1. **Visual Foundation**: Text predictions leverage *complete* visual context (like describing a photo only after seeing it in full).  \n",
    "\n",
    "2. **Unified Processing**: Predicts next image patches (e.g., reconstructing a photoâ€™s bottom half from the top), then generates text autoregressively (e.g., completing \"A dog plays in...\" â†’ \"park\").  \n",
    "\n",
    "3. **Vision-Centric Design**: Forces robust visual representations to support both image reconstruction *and* text generation.  \n",
    "\n",
    "\n",
    "I've written an in-depth blog about AIMv2, which you can read [here](https://medium.com/voxel51/visual-understanding-with-aimv2-76c58dcd68f9).\n",
    "\n",
    "#### **How AIMv2 Differs from CLIP**  \n",
    "\n",
    "I won't get into details about the CLIP family of models, but if you're interested in learning more [check out this blog](https://medium.com/voxel51/a-history-of-clip-model-training-data-advances-599473b48e1b). I do, however, want to briefly summarize the core differences between AIMv2 and CLIP:\n",
    "\n",
    "| **AIMv2** | **CLIP** |  \n",
    "|-----------|----------|  \n",
    "| Uses **autoregressive modeling** to reconstruct inputs *sequentially* (image patches â†’ text tokens) | Uses **contrastive learning** to align *parallel* image-text pairs |  \n",
    "| Processes images and text as a **unified sequence** | Processes modalities **separately** |  \n",
    "| Extracts training signals from **every token** (dense supervision) | Relies on **positive/negative pair contrast** (sparse supervision) |  \n",
    "| Requires **no specialized batch processing** | Demands **large batches** for effective negative sampling |  \n",
    "| Learns **implicit relationships** via sequential prediction | Forces **explicit alignment** of embeddings |  \n",
    "\n",
    "\n",
    "## Using AIMv2 in FiftyOne\n",
    "\n",
    "I've integrated AIMv2 in two plugins:\n",
    "\n",
    "1. [Zero-shot prediction plugin](https://github.com/jacobmarks/zero-shot-prediction-plugin)\n",
    "\n",
    "2. [AIMv2 embeddings plugin](https://github.com/harpreetsahota204/aim-embeddings-plugin)\n",
    "\n",
    "Letâ€™s begin with embeddings.\n",
    "\n",
    "#### Feature Extraction and Embedding Visualization in FiftyOne\n",
    "\n",
    "First, you'll need to install the plugin:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download https://github.com/harpreetsahota204/aim-embeddings-plugin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a dataset and plugins downloaded, weâ€™re ready to rock. \n",
    "\n",
    "You can, of course, use the plugin via the app. To learn how to do that you can refer to [the blog I wrote about the AIMv2 models](https://medium.com/voxel51/visual-understanding-with-aimv2-76c58dcd68f9) or follow the instructions on the [plugin's GitHub repo](https://github.com/harpreetsahota204/aim-embeddings-plugin).\n",
    "\n",
    "In this tutorial, however, we're going to stick to using the FiftyOne SDK. So, we need to instantiate an operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version is above 3.10, patching the collections module.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/harpreet/miniconda3/envs/fiftyone/lib/python3.11/site-packages/transformers/models/auto/image_processing_auto.py:595: FutureWarning: The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import fiftyone.operators as foo\n",
    "\n",
    "aim_embeddings = foo.get_operator(\"@harpreetsahota/aimv2_embeddings/compute_aimv2_embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the operator on your dataset\n",
    "\n",
    "You can choose from any model in the AIMv2 collection. [See the README](https://github.com/harpreetsahota204/aim-embeddings-plugin?tab=readme-ov-file#supported-models) on the plugin's repo for details. In this tutorial, we'll use [`apple/aimv2-large-patch14-224`](https://huggingface.co/apple/aimv2-large-patch14-224).\n",
    "\n",
    "The plugin supports two types of embeddings:\n",
    "\n",
    "- **Class Token Embedding (`cls`):** A single embedding vector derived from the special classification token. This represents the global semantic context of an image.\n",
    "\n",
    "- **Mean Pooling Embedding (`mean`):** An embedding vector computed by averaging the representations of all image patches. This captures distributed contextual information across the entire input.\n",
    "\n",
    "We'll compute embeddings using both methods. Iâ€™ll assume that youâ€™re running this in a Jupyter notebook, in which case you can run the entire model on the dataset as shown below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA device: NVIDIA RTX 6000 Ada Generation\n",
      " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4835/4835 [2.6m elapsed, 0s remaining, 30.7 samples/s]      \n",
      "Using CUDA device: NVIDIA RTX 6000 Ada Generation\n",
      " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4835/4835 [2.6m elapsed, 0s remaining, 31.3 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "embedding_types = ['cls', 'mean']\n",
    "\n",
    "for emb_type in embedding_types:\n",
    "  await aim_embeddings(\n",
    "      dataset,\n",
    "      model_name=\"apple/aimv2-large-patch14-224\",\n",
    "      embedding_types=emb_type,\n",
    "      emb_field=f\"aimv2_{emb_type}_emb\",\n",
    "      delegate=True\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll visualize these embeddings shortly, and before we do let's go ahead and compute embeddings using the CLIP model as well. This way we can compare how both models represent and organize the same images in their respective embedding spaces.\n",
    "\n",
    "We can use the CLIP model from the FiftyOne model zoo (which is the same as one of the models they assessed in the ImageNet-D paper). You'll notice that I'm instantiating the model with the `classes` and `text_prompt` argument, that's because we will use the model for zero-shot classification later. The presence of these arguments won't impact the embeddings that we get as these are computed based only the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "clip_model = foz.load_zoo_model(\n",
    "    \"clip-vit-base32-torch\",\n",
    "    text_prompt=\"A photo of a\",\n",
    "    classes=gt_labels,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `clip_model` instantiated we can use the [`compute_embeddings`](https://docs.voxel51.com/api/fiftyone.core.dataset.html?highlight=compute_embeddings#fiftyone.core.dataset.Dataset.compute_embeddings) method of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4835/4835 [25.6s elapsed, 0s remaining, 181.2 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "dataset.compute_embeddings(\n",
    "    model=clip_model,\n",
    "    embeddings_field=\"clip_emb\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ðŸ¤” You're probably wondering why we're using one pattern for computing embeddings with AIMv2 (e.g. using a plugin) and another one to compute embeddings with CLIP (e.g. using a model from the model zoo).\n",
    "\n",
    "That's a fair question!\n",
    "\n",
    "FiftyOne has a powerful [plugins framework](https://docs.voxel51.com/plugins/index.html#getting-started) that allows you to extend the functionality of the library without changing the core code, submitting a PR, and then having to wait for the PR to get merged. It's a way to incorporate cutting edge models and methods into your workflow at the speed of you. We host monthly workshops that teach you about the plugin ecosystem, how to use it in your workflow, and also how to develop them. \n",
    "\n",
    "You can [check our events calendar for the next workshop](https://voxel51.com/computer-vision-events/), just search for the event titled *Advanced Computer Vision Data Curation and Model Evaluation*.\n",
    "\n",
    "### Visualizing embeddings\n",
    "\n",
    "Now that we've computed embeddings, we can visualize them. To do this, we need to project our high dimensional embeddings to two dimensions. For this we can use [UMAP](https://umap-learn.readthedocs.io/en/latest/).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating visualization...\n",
      "UMAP(min_dist=0.051, verbose=True)\n",
      "Thu Feb 27 06:55:24 2025 Construct fuzzy simplicial set\n",
      "Thu Feb 27 06:55:24 2025 Finding Nearest Neighbors\n",
      "Thu Feb 27 06:55:24 2025 Building RP forest with 8 trees\n",
      "Thu Feb 27 06:55:27 2025 NN descent for 12 iterations\n",
      "\t 1  /  12\n",
      "\t 2  /  12\n",
      "\t 3  /  12\n",
      "\t 4  /  12\n",
      "\t 5  /  12\n",
      "\tStopping threshold met -- exiting after 5 iterations\n",
      "Thu Feb 27 06:55:38 2025 Finished Nearest Neighbor Search\n",
      "Thu Feb 27 06:55:40 2025 Construct embedding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01e06e20f55549e4bd906e15120d933f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs completed:   0%|            0/500 [00:00]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcompleted  0  /  500 epochs\n",
      "\tcompleted  50  /  500 epochs\n",
      "\tcompleted  100  /  500 epochs\n",
      "\tcompleted  150  /  500 epochs\n",
      "\tcompleted  200  /  500 epochs\n",
      "\tcompleted  250  /  500 epochs\n",
      "\tcompleted  300  /  500 epochs\n",
      "\tcompleted  350  /  500 epochs\n",
      "\tcompleted  400  /  500 epochs\n",
      "\tcompleted  450  /  500 epochs\n",
      "Thu Feb 27 06:55:42 2025 Finished embedding\n",
      "Generating visualization...\n",
      "UMAP(min_dist=0.051, verbose=True)\n",
      "Thu Feb 27 06:55:42 2025 Construct fuzzy simplicial set\n",
      "Thu Feb 27 06:55:42 2025 Finding Nearest Neighbors\n",
      "Thu Feb 27 06:55:42 2025 Building RP forest with 8 trees\n",
      "Thu Feb 27 06:55:42 2025 NN descent for 12 iterations\n",
      "\t 1  /  12\n",
      "\t 2  /  12\n",
      "\t 3  /  12\n",
      "\t 4  /  12\n",
      "\tStopping threshold met -- exiting after 4 iterations\n",
      "Thu Feb 27 06:55:42 2025 Finished Nearest Neighbor Search\n",
      "Thu Feb 27 06:55:42 2025 Construct embedding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2879cf5ba33f40088003d9de7e763e6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs completed:   0%|            0/500 [00:00]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcompleted  0  /  500 epochs\n",
      "\tcompleted  50  /  500 epochs\n",
      "\tcompleted  100  /  500 epochs\n",
      "\tcompleted  150  /  500 epochs\n",
      "\tcompleted  200  /  500 epochs\n",
      "\tcompleted  250  /  500 epochs\n",
      "\tcompleted  300  /  500 epochs\n",
      "\tcompleted  350  /  500 epochs\n",
      "\tcompleted  400  /  500 epochs\n",
      "\tcompleted  450  /  500 epochs\n",
      "Thu Feb 27 06:55:43 2025 Finished embedding\n",
      "Generating visualization...\n",
      "UMAP(min_dist=0.051, verbose=True)\n",
      "Thu Feb 27 06:55:43 2025 Construct fuzzy simplicial set\n",
      "Thu Feb 27 06:55:43 2025 Finding Nearest Neighbors\n",
      "Thu Feb 27 06:55:43 2025 Building RP forest with 8 trees\n",
      "Thu Feb 27 06:55:43 2025 NN descent for 12 iterations\n",
      "\t 1  /  12\n",
      "\t 2  /  12\n",
      "\t 3  /  12\n",
      "\t 4  /  12\n",
      "\t 5  /  12\n",
      "\tStopping threshold met -- exiting after 5 iterations\n",
      "Thu Feb 27 06:55:44 2025 Finished Nearest Neighbor Search\n",
      "Thu Feb 27 06:55:44 2025 Construct embedding\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91fb3d4a089c44dfbe772c573d5e3758",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epochs completed:   0%|            0/500 [00:00]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tcompleted  0  /  500 epochs\n",
      "\tcompleted  50  /  500 epochs\n",
      "\tcompleted  100  /  500 epochs\n",
      "\tcompleted  150  /  500 epochs\n",
      "\tcompleted  200  /  500 epochs\n",
      "\tcompleted  250  /  500 epochs\n",
      "\tcompleted  300  /  500 epochs\n",
      "\tcompleted  350  /  500 epochs\n",
      "\tcompleted  400  /  500 epochs\n",
      "\tcompleted  450  /  500 epochs\n",
      "Thu Feb 27 06:55:44 2025 Finished embedding\n"
     ]
    }
   ],
   "source": [
    "import fiftyone.brain as fob\n",
    "\n",
    "embedding_fields = [\"aimv2_cls_emb\", \"aimv2_mean_emb\", \"clip_emb\"]\n",
    "\n",
    "for embeddings in embedding_fields:\n",
    "  results = fob.compute_visualization(\n",
    "      dataset,\n",
    "      embeddings=embeddings,\n",
    "      method=\"umap\",\n",
    "      brain_key=f\"{embeddings}_viz\",\n",
    "      num_dims=2,\n",
    "      n_neighbors=10,\n",
    "      min_dist=0.051,\n",
    "      verbose=True,\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you launch the app, take some time to explore how each model organizes these synthetically generated images in its embedding space. \n",
    "\n",
    "Since ImageNet-D systematically varies backgrounds, materials, and textures for each object category, pay special attention to whether the models cluster images based on the core object category (C) or if they're distracted by the intentionally introduced nuisance factors (N). For instance, do images of the same object with different backgrounds cluster together, suggesting the model has learned robust object recognition, or do they scatter based on background similarities? \n",
    "\n",
    "Look for interesting patterns like whether AIMv2's autoregressive approach is more resilient to these synthetic variations compared to CLIP's contrastive learning. You might notice that one model creates clusters that better preserve semantic object categories despite varying textures and materials, while the other might be more influenced by surface-level visual similarities.  Try filtering by specific classes and examining how well the models handle extreme variations - for example, do common objects remain well-clustered even when rendered with unusual materials or placed in unexpected contexts? \n",
    "\n",
    "These patterns can reveal deeper insights about each model's robustness to synthetic perturbations and their ability to distinguish between essential object features and artificially introduced variations.\n",
    "\n",
    "I'm curious if you find any interesting patterns, examples, or insight. If so, comment below!\n",
    "\n",
    "![ImageNet-D Examples](assets/imagenet-d-embeddings.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-Shot Classification using in FiftyOne\n",
    "\n",
    "To get started, let's download the [zero-shot prediction plugin](https://github.com/jacobmarks/zero-shot-prediction-plugin) and instantiate the operator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download https://github.com/jacobmarks/zero-shot-prediction-plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.operators as foo\n",
    "\n",
    "zsc = foo.get_operator(\"@jacobmarks/zero_shot_prediction/zero_shot_classify\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although there were several checkpoints and sizes of feature extractors that were released as part of the AIMv2 collection, only one has been made available for zero-shot classification, [`aimv2-large-patch14-224-lit`](https://huggingface.co/apple/aimv2-large-patch14-224-lit). This is the model that is used in the zero-shot prediction plugin.  You'll recall that earlier we got parsed the ground truth labels to list `gt_labels`. [Under the hood](https://github.com/jacobmarks/zero-shot-prediction-plugin/blob/d85a71c17a9d8a65a5bb1913054347750e6e93f9/classification.py#L382) we are parsing each of the classes into in the required prompt of `Picture of a {category}`, and AIMv2 will select the one with the high probability as the prediction.\n",
    "\n",
    "The pattern for using this plugin via the SDK is the same as we saw above, we pass in the required arguments to the operator and wait."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fiftyone.operators.executor.ExecutionResult at 0x7006beb17c90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await zsc(\n",
    "    dataset,\n",
    "    labels=gt_labels,\n",
    "    model_name=\"AIMv2\",\n",
    "    label_field=\"AIMv2_predictions\",\n",
    "    delegate=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also CLIP for zero-shot detection. Recall  that when we instantiated the `clip_model` we did so with the list of `gt_classes` and the required prefix prompt `A photo of a`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4835/4835 [24.7s elapsed, 0s remaining, 190.2 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "dataset.apply_model(\n",
    "    model=clip_model, \n",
    "    label_field=\"clip_predictions\",\n",
    "    store_logits=True\n",
    "    )\n",
    "\n",
    "# Save the additions we've made to the database\n",
    "\n",
    "dataset.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation\n",
    "\n",
    "You can use the [`evaluate_classifications`](https://docs.voxel51.com/api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.evaluate_classifications) method to evaluate the predictions of the zero-shot classifiers, this will return a [`ClassificationResults`](https://docs.voxel51.com/api/fiftyone.utils.eval.classification.html#fiftyone.utils.eval.classification.ClassificationResults) instance that provides a variety of methods for generating various aggregate evaluation reports about your model.\n",
    "\n",
    "By default, the classifications will be treated as a generic multiclass classification task, and for illustration purposes I am explicitly requesting that `simple` evaluation be used by setting the method parameter to \"simple\"; but you can specify other evaluation strategies such as [`top-k`](https://docs.voxel51.com/user_guide/evaluation.html#top-k-evaluation) accuracy or [`binary`](https://docs.voxel51.com/user_guide/evaluation.html#binary-evaluation) evaluation via the method parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "zsc_preds = [\"AIMv2_predictions\", \"clip_predictions\"]\n",
    "\n",
    "for pred in zsc_preds:\n",
    "    __key = pred.split(\"_\")[0]\n",
    "    dataset.evaluate_classifications(\n",
    "        pred_field=pred,\n",
    "        gt_field=\"ground_truth\",\n",
    "        method=\"simple\",\n",
    "        eval_key=f\"{__key}_simple_eval\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the `evaluate_classifications` method has completed, you can analyze the results right in the app with the [Model Evaluation panel](https://docs.voxel51.com/user_guide/app.html#app-model-evaluation-panel). \n",
    "\n",
    "With this panel you can analyze the performance of each individually:\n",
    "\n",
    "![ImageNet-D Examples](assets/imagenet-d-model-eval.gif)\n",
    "\n",
    "\n",
    " #### Or you can compare the performance against each other:\n",
    "\n",
    " ![ImageNet-D Examples](assets/imagenet-d-compare-models.gif)\n",
    "\n",
    " Note that the results displayed are the micro-averaged results. In multiclass classification, when using micro-averaging, precision, recall, and F1 score will have the same value, and this value will be equal to the accuracy.\n",
    "\n",
    " You can also access the results of the evaluation via the SDK:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "aim_eval_results = dataset.load_evaluation_results(\"AIMv2_simple_eval\")\n",
    "\n",
    "clip_eval_results = dataset.load_evaluation_results(\"clip_simple_eval\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A Brief Refresher\n",
    "\n",
    "**Accuracy** in multiclass classification, accuracy is the ratio of correctly classified instances to the total number of instances. It gives an overall sense of how well the classifier is performing across all classes.\n",
    "\n",
    "*   **Precision, Recall, and F1-score** In multiclass classification, precision, recall, and F1-score can be calculated in several ways:\n",
    "\n",
    "    *   **Micro-averaging:** Calculate metrics globally by counting the total true positives, false negatives, and false positives.\n",
    "\n",
    "    *   **Macro-averaging:** Calculate metrics for each class and then average them. This gives equal weight to each class.\n",
    "    \n",
    "    *   **Weighted-averaging:** Calculate metrics for each class and average them, weighting each class by its support (number of true instances for each label).\n",
    "\n",
    "For brevity, let's explore only the results for `weighted`. Note, you can run the following for a classwise breakdown of the model performance:\n",
    "\n",
    "```python\n",
    "aim_eval_results.print_report()\n",
    "```\n",
    "\n",
    "In Table 3 of the [AIMv2 paper](https://arxiv.org/pdf/2403.18775), the authors reported the accuracy for CLIP ViT-B/32 as across the whole of ImageNet-D as 21.96. As you can see below, we observe similar performance with an accuracy of 25.07.\n",
    "\n",
    "However, what stands out is the peformance of AIMv2 which has a top-line accuracy of 41.92 and is just mopping the floor with CLIP across the other metrics! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy   0.4192\n",
      "precision  0.5996\n",
      "recall     0.4192\n",
      "fscore     0.451\n",
      "support    4835\n"
     ]
    }
   ],
   "source": [
    "aim_eval_results.print_metrics(average='weighted', digits=4) # you can also pass in \"micro\" or \"macro\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy   0.2507\n",
      "precision  0.4637\n",
      "recall     0.2507\n",
      "fscore     0.2856\n",
      "support    4835\n"
     ]
    }
   ],
   "source": [
    "clip_eval_results.print_metrics(average='weighted', digits=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the Hardest Samples\n",
    "\n",
    "The FiftyOne Brain provides a hardness measure that calculates how easy or difficult it is for your model to understand any given sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing hardness...\n",
      " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4835/4835 [4.4s elapsed, 0s remaining, 1.1K samples/s]       \n",
      "Hardness computation complete\n",
      "Computing hardness...\n",
      " 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4835/4835 [4.4s elapsed, 0s remaining, 1.1K samples/s]       \n",
      "Hardness computation complete\n"
     ]
    }
   ],
   "source": [
    "import fiftyone.brain as fob\n",
    "\n",
    "zsc_preds = [\"AIMv2_predictions\", \"clip_predictions\"]\n",
    "\n",
    "for pred in zsc_preds:\n",
    "    fob.compute_hardness(dataset, label_field=pred, hardness_field=f\"{pred}_hardness\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ImageNet-D Examples](assets/imagenet-d-hardness.gif)\n",
    "\n",
    "\n",
    "The concept of hardness is particularly interesting for ImageNet-D because:\n",
    "\n",
    "1. **By Design Difficulty**: ImageNet-D was specifically created through a \"hard image mining\" strategy where images were only included if they fooled a set of surrogate models. So in a sense, every image in the dataset was already selected for being \"hard.\"\n",
    "\n",
    "\n",
    "2. **Layered Hardness**: Computing hardness scores on these already-hard images can reveal which synthetic variations are especially challenging for our specific models (AIMv2 and CLIP). This gives us a \"hardness within hardness\" perspective.\n",
    "\n",
    "\n",
    "Key questions to investigate:\n",
    "\n",
    "- For samples that are \"hard\" for one model but \"easy\" for another, what characteristics distinguish them?\n",
    "\n",
    "\n",
    "- Is there a relationship between embedding cluster position and hardness? Do the hardest samples tend to lie in particular regions of the embedding space?\n",
    "\n",
    "\n",
    "This analysis could reveal valuable insights about whether certain architectural choices (autoregressive vs contrastive) make models more robust to specific types of synthetic perturbations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "\n",
    "You'll notice that I have given you the tools to understand, explore, and analyze the peformance of AIMv2 and CLIP on ImageNet-D; but I haven't given you any answers. That's because I want you to take some time and explore it on your own!\n",
    "\n",
    "After you've taken the time to dig deeper into the dataset and model performance, here's what you can do to level up your analysis (and FiftyOne skills)\n",
    "\n",
    "- Explore one of the other checkpoints for feature extraction using the AIMv2 embeddings plugin, a good place to start is [`aimv2-large-patch14-native`](https://huggingface.co/apple/aimv2-large-patch14-native).\n",
    "\n",
    "- Since you've already computed embeddings in this tutorial, you can use them in the FiftyOne Brain to [compute uniqueness values](https://docs.voxel51.com/brain.html#brain-image-uniqueness) for each sample.\n",
    "\n",
    "- Likewise you can [compute representativeness](https://docs.voxel51.com/brain.html#brain-image-representativeness) values to find samples which are very similar to large clusters of your the entire ImageNet-D dataset.\n",
    "\n",
    "- Use the [Janus Pro](https://github.com/harpreetsahota204/janus-vqa-fiftyone) or the [Moondream2](https://github.com/harpreetsahota204/moondream2-plugin) plugin with the prompt `What is the main object\n",
    "in this image? Respond with one word only` and  repeat the evaluation as we did in this blog.\n",
    "\n",
    "If you have any questions or want to stay up to date with us at FiftyOne, feel free to join our [Discord community](https://discord.com/invite/fiftyone-community)!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
