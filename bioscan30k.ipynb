{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIOSCAN-5M\n",
    "\n",
    "The BIOSCAN-5M dataset is a multimodal collection of over 5.15 million arthropod specimens (98% insects), curated to advance biodiversity monitoring through machine learning. It expands the earlier BIOSCAN-1M dataset by including high-resolution images, DNA barcode sequences, taxonomic labels (phylum to species), geographical locations, specimen size data, and Barcode Index Numbers (BINs). Designed for both closed-world (known species) and open-world (novel species) scenarios, it supports tasks like taxonomic classification, clustering, and multimodal learning.\n",
    "\n",
    "##### This dataset is a randomly chosen subset of 30,000 samples across all splits from the Cropped 256 dataset\n",
    "\n",
    "Key Features:\n",
    "\n",
    "* Images: 5.15M high-resolution microscope images (1024√ó768px) with cropped/resized versions.\n",
    "\n",
    "* Genetic Data: Raw DNA barcode sequences (COI gene) and BIN clusters.\n",
    "\n",
    "* Taxonomy: Labels for 7 taxonomic ranks (phylum, class, order, family, subfamily, genus, species).\n",
    "\n",
    "* Geographical Metadata: Collection country, province/state, latitude/longitude.\n",
    "\n",
    "* Size Metadata: Pixel count, area fraction, and scale factor for specimens\n",
    "\n",
    "Let's begin by installing some dependencies and [downloading the dataset from the Voxel51 org on Hugging Face](https://huggingface.co/datasets/Voxel51/BIOSCAN-30k)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fiftyone open-clip-torch umap-learn transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone.utils.huggingface import load_from_hub\n",
    "\n",
    "dataset = load_from_hub(\n",
    "    \"Voxel51/BIOSCAN-30k\",\n",
    "    name=\"bioscan30k\",\n",
    "    overwrite=True,\n",
    "    persistent=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has geolocation, to visualize that in the FiftyOne app you'll need to a Mapbox key. You can sign up for a key [here](https://account.mapbox.com/auth/signup/), it's free and you get 50,000 free map loads. Once you have a Mapbox account and API key, you will need to set the following environment variable `export MAPBOX_TOKEN=xxxxxxx`. Alternatively, if you're running this in a Jupyter notebook you can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"MAPBOX_TOKEN\"] = getpass(\"Input your Mapbox token:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let‚Äôs install a plugin that allows us to create custom dashboards and glean more insight into our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download \\\n",
    "    https://github.com/voxel51/fiftyone-plugins \\\n",
    "    --plugin-names @voxel51/dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the dataset has been downloaded you can begin exploring it in the FiftyOne app. Once the dataset has been downloaded, you can do some initial exploration by launching the app.\n",
    "\n",
    "There are two ways to use the app:\n",
    "\n",
    "* As a cell in your notebook, which you can do by running `fo.launch_app(dataset)`\n",
    "\n",
    "* In a separate browser window, run `fiftyone app launch` in your terminal\n",
    "\n",
    "Once the app is launched, you can explore the dataset by:\n",
    "\n",
    "* Scrolling through the images for a visual vibecheck of its contents\n",
    "\n",
    "* Filter based on the labels (the various taxonomic classifications, geographic information, or size measurements)\n",
    "\n",
    "* Opening the map panel and exploring based on geographic location\n",
    "\n",
    "* Create a dashboard of plots for the various information fields of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üêõ Warning: You're about to see some creepy crawly insects.\n",
    "\n",
    "Below is an example of using the map panel:\n",
    "\n",
    "![Explore bioscal](assets/bioscan-explore.gif)\n",
    "\n",
    "\n",
    "#### You can also create a custom dashboard like so:\n",
    "\n",
    "![Explore bioscal](assets/bioscan-5m-dashboard.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can call the dataset as shown below to see all the fields available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deeper analysis with FiftyOne\n",
    "\n",
    "You can take your analysis to a deeper level by using embeddings based workflows. \n",
    "\n",
    "The authors of the paper mentioned they trained a CLIP like model. This model, built using the CLIBD (Contrastive Learning for Image-Barcode-Description) framework, learns a shared embedding space across the three modalities, enabling cross-modal queries and improving performance in taxonomic classification tasks. However, I was unable to find the model weights on Hugging Face or through the projects GitHub repo. \n",
    "\n",
    "Instead, I will make use of some other models which were mentioned in the paper.\n",
    "\n",
    "Note: I'm not an expert in biology, genomics, or insects. I'm just a hacker. I apologize in advance to the community of pracitioners working in this space if I'm not using the models as intended. My goal is to to show you what's possible when you use the open source FiftyOne library. \n",
    "\n",
    "Let's start computing embeddings for the images using [BioCLIP](https://github.com/Imageomics/bioclip/tree/main).\n",
    "\n",
    "BioCLIP extends the CLIP framework to create a vision foundation model specialized for biological imagery, focusing on taxonomic relationships across the tree of life. Trained on TreeOfLife-10M‚Äîa novel dataset of 10M biological images spanning 454K taxa ‚Äî BioCLIP learns hierarchical representations aligned with taxonomic ranks (kingdom to species). Unlike standard CLIP, it treats species as interconnected nodes in a biological hierarchy rather than isolated classes.\n",
    "\n",
    "BioCLIP is part of the [Open CLIP](https://github.com/mlfoundations/open_clip) ecosystem, so you can use FiftyOne's integration with as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.zoo as foz\n",
    "\n",
    "bio_clip_model = foz.load_zoo_model(\n",
    "    \"open-clip-torch\",\n",
    "    pretrained=\"\",\n",
    "    clip_model=\"hf-hub:imageomics/bioclip\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is downloaded, you can compute embeddings as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\" #use GPU if available\n",
    "\n",
    "dataset.compute_embeddings(\n",
    "    model=bio_clip_model,\n",
    "    embeddings_field=\"bio_clip_embeddings\",\n",
    "    batch_size=128, #use whatever batch size your GPU can handle\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll visualize these embeddings shortly, but first let's compute embeddings for the DNA Sequences using [BarcodeBERT](https://github.com/bioscan-ml/BarcodeBERT).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, BertConfig\n",
    "\n",
    "# First load the configuration\n",
    "barcode_bert_config = BertConfig.from_pretrained(\n",
    "    \"bioscan-ml/BarcodeBERT\", \n",
    "    trust_remote_code=True\n",
    "    )\n",
    "\n",
    "# Load the tokenizer\n",
    "barcode_bert_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"bioscan-ml/BarcodeBERT\", \n",
    "    trust_remote_code=True\n",
    "    )\n",
    "\n",
    "# Load the model\n",
    "barcode_bert_model = AutoModel.from_pretrained(\n",
    "    \"bioscan-ml/BarcodeBERT\", \n",
    "    device_map=device,\n",
    "    trust_remote_code=True,\n",
    "    config=barcode_bert_config\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for sample in dataset:\n",
    "        dna_sequence = sample[\"dna_barcode\"]['value']\n",
    "        inputs = barcode_bert_tokenizer(dna_sequence, return_tensors=\"pt\")[\"input_ids\"]\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = barcode_bert_model(inputs.unsqueeze(0))[\"hidden_states\"][-1]\n",
    "        embs = outputs.mean(1).squeeze().cpu().numpy()\n",
    "        sample[\"barcode_bert_embeddings\"] = embs\n",
    "        sample.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can visualize the embeddings!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.brain as fob\n",
    "\n",
    "embedding_fields = [ \"bio_clip_embeddings\", \"barcode_bert_embeddings\"]\n",
    "\n",
    "for fields in embedding_fields:\n",
    "    _fname = fields.split(\"_embeddings\")[0]\n",
    "    results = fob.compute_visualization(\n",
    "        dataset,\n",
    "        embeddings=fields,\n",
    "        method=\"umap\",\n",
    "        brain_key=f\"{_fname}_viz\",\n",
    "        num_dims=2,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "fiftyone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
