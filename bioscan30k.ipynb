{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIOSCAN-5M\n",
    "\n",
    "The BIOSCAN-5M dataset is a multimodal collection of over 5.15 million arthropod specimens (98% insects), curated to advance biodiversity monitoring through machine learning. It expands the earlier BIOSCAN-1M dataset by including high-resolution images, DNA barcode sequences, taxonomic labels (phylum to species), geographical locations, specimen size data, and Barcode Index Numbers (BINs). Designed for both closed-world (known species) and open-world (novel species) scenarios, it supports tasks like taxonomic classification, clustering, and multimodal learning.\n",
    "\n",
    "##### This dataset is a randomly chosen subset of 30,000 samples across all splits from the Cropped 256 dataset\n",
    "\n",
    "Key Features:\n",
    "\n",
    "* Images: 5.15M high-resolution microscope images (1024√ó768px) with cropped/resized versions.\n",
    "\n",
    "* Genetic Data: Raw DNA barcode sequences (COI gene) and BIN clusters.\n",
    "\n",
    "* Taxonomy: Labels for 7 taxonomic ranks (phylum, class, order, family, subfamily, genus, species).\n",
    "\n",
    "* Geographical Metadata: Collection country, province/state, latitude/longitude.\n",
    "\n",
    "* Size Metadata: Pixel count, area fraction, and scale factor for specimens\n",
    "\n",
    "Let's begin by installing some dependencies and [downloading the dataset from the Voxel51 org on Hugging Face](https://huggingface.co/datasets/Voxel51/BIOSCAN-30k)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fiftyone open-clip-torch umap-learn transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone.utils.huggingface import load_from_hub\n",
    "\n",
    "dataset = load_from_hub(\n",
    "    \"Voxel51/BIOSCAN-30k\",\n",
    "    name=\"bioscan30k\",\n",
    "    overwrite=True,\n",
    "    persistent=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has geolocation, to visualize that in the FiftyOne app you'll need to a Mapbox key. You can sign up for a key [here](https://account.mapbox.com/auth/signup/), it's free and you get 50,000 free map loads. Once you have a Mapbox account and API key, you will need to set the following environment variable `export MAPBOX_TOKEN=xxxxxxx`. Alternatively, if you're running this in a Jupyter notebook you can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from getpass import getpass\n",
    "\n",
    "os.environ[\"MAPBOX_TOKEN\"] = getpass(\"Input your Mapbox token:\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let‚Äôs install a plugin that allows us to create custom dashboards and glean more insight into our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download \\\n",
    "    https://github.com/voxel51/fiftyone-plugins \\\n",
    "    --plugin-names @voxel51/dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the dataset has been downloaded you can begin exploring it in the FiftyOne app. Once the dataset has been downloaded, you can do some initial exploration by launching the app.\n",
    "\n",
    "There are two ways to use the app:\n",
    "\n",
    "* As a cell in your notebook, which you can do by running `fo.launch_app(dataset)`\n",
    "\n",
    "* In a separate browser window, run `fiftyone app launch` in your terminal\n",
    "\n",
    "Once the app is launched, you can explore the dataset by:\n",
    "\n",
    "* Scrolling through the images for a visual vibecheck of its contents\n",
    "\n",
    "* Filter based on the labels (the various taxonomic classifications, geographic information, or size measurements)\n",
    "\n",
    "* Opening the map panel and exploring based on geographic location\n",
    "\n",
    "* Create a dashboard of plots for the various information fields of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### üêõ Warning: You're about to see some creepy crawly insects.\n",
    "\n",
    "Below is an example of using the map panel:\n",
    "\n",
    "![Explore bioscan](assets/bioscan-explore.gif)\n",
    "\n",
    "\n",
    "#### You can also create a custom dashboard like so:\n",
    "\n",
    "![Explore bioscan](assets/bioscan-5m-dashboard.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can call the dataset as shown below to see all the fields available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deeper analysis with FiftyOne\n",
    "\n",
    "You can take your analysis to a deeper level by using embeddings based workflows. \n",
    "\n",
    "The authors of the paper mentioned they trained a CLIP like model. This model, built using the CLIBD (Contrastive Learning for Image-Barcode-Description) framework, learns a shared embedding space across the three modalities, enabling cross-modal queries and improving performance in taxonomic classification tasks. However, I was unable to find the model weights on Hugging Face or through the projects GitHub repo. \n",
    "\n",
    "Instead, I will make use of some other models which were mentioned in the paper.\n",
    "\n",
    "Note: I'm not an expert in biology, genomics, or insects. I'm just a hacker. I apologize in advance to the community of pracitioners working in this space if I'm not using the models as intended. My goal is to to show you what's possible when you use the open source FiftyOne library. \n",
    "\n",
    "Let's start computing embeddings for the images using [BioCLIP](https://github.com/Imageomics/bioclip/tree/main).\n",
    "\n",
    "BioCLIP extends the CLIP framework to create a vision foundation model specialized for biological imagery, focusing on taxonomic relationships across the tree of life. Trained on TreeOfLife-10M‚Äîa novel dataset of 10M biological images spanning 454K taxa ‚Äî BioCLIP learns hierarchical representations aligned with taxonomic ranks (kingdom to species). Unlike standard CLIP, it treats species as interconnected nodes in a biological hierarchy rather than isolated classes.\n",
    "\n",
    "BioCLIP is part of the [Open CLIP](https://github.com/mlfoundations/open_clip) ecosystem, so you can use FiftyOne's integration with as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.zoo as foz\n",
    "\n",
    "bio_clip_model = foz.load_zoo_model(\n",
    "    \"open-clip-torch\",\n",
    "    pretrained=\"\",\n",
    "    clip_model=\"hf-hub:imageomics/bioclip\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is downloaded, you can compute embeddings as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "device=\"cuda\" if torch.cuda.is_available() else \"cpu\" #use GPU if available\n",
    "\n",
    "dataset.compute_embeddings(\n",
    "    model=bio_clip_model,\n",
    "    embeddings_field=\"bio_clip_embeddings\",\n",
    "    batch_size=128, #use whatever batch size your GPU can handle\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll visualize these embeddings shortly, but first let's compute embeddings for the DNA Sequences using [BarcodeBERT](https://github.com/bioscan-ml/BarcodeBERT).\n",
    "\n",
    "BarcodeBERT is a specialized transformer model designed for biodiversity analysis through DNA barcode sequences. Built on the BERT architecture, it adapts self-supervised pretraining to the unique demands of taxonomic classification, particularly for invertebrates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel, BertConfig\n",
    "\n",
    "# First load the configuration\n",
    "barcode_bert_config = BertConfig.from_pretrained(\n",
    "    \"bioscan-ml/BarcodeBERT\", \n",
    "    trust_remote_code=True\n",
    "    )\n",
    "\n",
    "# Load the tokenizer\n",
    "barcode_bert_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"bioscan-ml/BarcodeBERT\", \n",
    "    trust_remote_code=True\n",
    "    )\n",
    "\n",
    "# Load the model\n",
    "barcode_bert_model = AutoModel.from_pretrained(\n",
    "    \"bioscan-ml/BarcodeBERT\", \n",
    "    device_map=device,\n",
    "    trust_remote_code=True,\n",
    "    config=barcode_bert_config\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    for sample in dataset:\n",
    "        dna_sequence = sample[\"dna_barcode\"]['value']\n",
    "        inputs = barcode_bert_tokenizer(dna_sequence, return_tensors=\"pt\")[\"input_ids\"]\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = barcode_bert_model(inputs.unsqueeze(0))[\"hidden_states\"][-1]\n",
    "        embs = outputs.mean(1).squeeze().cpu().numpy()\n",
    "        sample[\"barcode_bert_embeddings\"] = embs\n",
    "        sample.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compute a 2D representation of our high-dimensional embeddings using UMAP. This will allow us to visualize how different specimens are related to each other in the embedding space while preserving the important relationships between data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.brain as fob\n",
    "\n",
    "embedding_fields = [ \"bio_clip_embeddings\", \"barcode_bert_embeddings\"]\n",
    "\n",
    "for fields in embedding_fields:\n",
    "    _fname = fields.split(\"_embeddings\")[0]\n",
    "    results = fob.compute_visualization(\n",
    "        dataset,\n",
    "        embeddings=fields,\n",
    "        method=\"umap\",\n",
    "        brain_key=f\"{_fname}_viz\",\n",
    "        num_dims=2,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can visualize our results in the app:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Explore bioscan](assets/bioscan-5m-embs.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using embeddings to gain deeper insights\n",
    "\n",
    "You can use the embeddings you've just computed to compute scores for [uniqueness](https://docs.voxel51.com/brain.html#image-uniqueness) and [representativeness](https://docs.voxel51.com/brain.html#image-representativeness).\n",
    "\n",
    "Uniqueness will compute a scalar-value for each sample that ranks the uniqueness of that sample (higher value means more unique). The uniqueness values for a dataset are normalized to [0, 1], with the most unique sample in the collection having a uniqueness value of 1. \n",
    "\n",
    "Representativeness will compute a scalar-value for each sample that ranks the representativeness of that sample (higher value means more representative). The representativeness values for a dataset are normalized to [0, 1], with the most representative samples in the collection having a representativeness value of 1.\n",
    "\n",
    "Since we have embeddings computed already for images and for the DNA barcode you can compute these values this for either field. In the examples below we'll compute them for the BioCLIP embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.brain as fob\n",
    "\n",
    "fob.compute_uniqueness(\n",
    "    samples=dataset,\n",
    "    uniqueness_field=\"bio_clip_uniqueness\",\n",
    "    embeddings=\"bio_clip_embeddings\",\n",
    ")\n",
    "\n",
    "fob.compute_representativeness(\n",
    "    samples=dataset,\n",
    "    representativeness_field=\"bio_clip_representativeness\",\n",
    "    embeddings=\"bio_clip_embeddings\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise you can compute the representativeness based on the Barcode BERT embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing uniqueness...\n",
      "Generating neighbors graph for 30000 embeddings...\n",
      "Index complete\n",
      "Uniqueness computation complete\n",
      "Computing representativeness...\n",
      "Computing clusters for 30000 embeddings; this may take awhile...\n",
      "Representativeness computation complete\n"
     ]
    }
   ],
   "source": [
    "import fiftyone.brain as fob\n",
    "\n",
    "fob.compute_uniqueness(\n",
    "    samples=dataset,\n",
    "    uniqueness_field=\"barcode_bert_uniqueness\",\n",
    "    embeddings=\"barcode_bert_embeddings\",\n",
    ")\n",
    "\n",
    "fob.compute_representativeness(\n",
    "    samples=dataset,\n",
    "    representativeness_field=\"barcode_bert_representativeness\",\n",
    "    embeddings=\"barcode_bert_embeddings\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once these fields have been computed you can interact with them in the app as you normally would:\n",
    "\n",
    "![Explore bioscan](assets/bioscan-5m-uniq-rep.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "In this exploration of the BIOSCAN-30k dataset, we've demonstrated several powerful ways to analyze biodiversity data using FiftyOne:\n",
    "\n",
    "1. **Data Exploration**: We explored the dataset's rich taxonomic, geographic, and specimen metadata through FiftyOne's visualization tools and custom dashboards.\n",
    "\n",
    "2. **Multimodal Analysis**: We leveraged two state-of-the-art models:\n",
    "   - BioCLIP for visual embeddings of specimen images\n",
    "   - BarcodeBERT for DNA barcode sequence embeddings\n",
    "\n",
    "3. **Advanced Analytics**: We used these embeddings to:\n",
    "   - Visualize relationships between specimens using UMAP dimensionality reduction\n",
    "   - Identify unique and representative samples in the dataset\n",
    "   - Create interactive visualizations for exploring specimen similarities\n",
    "\n",
    "This workflow showcases how modern ML tools can help researchers and practitioners analyze large-scale biodiversity datasets, potentially accelerating species identification and ecological research.\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "Some potential directions for further analysis:\n",
    "- Clustering analysis to identify specimen groups\n",
    "- Cross-modal similarity search (finding similar specimens across images and DNA sequences)\n",
    "- Geographic distribution analysis of specific taxonomic groups\n",
    "- Training custom models for automated species identification\n",
    "\n",
    "For more information about the BIOSCAN project and dataset, visit:\n",
    "- [BIOSCAN Project Page](https://biodiversitygenomics.net/projects/5m-insects/)\n",
    "- [Dataset Paper](https://arxiv.org/abs/2406.12723)\n",
    "- [FiftyOne Documentation](https://docs.voxel51.com/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
