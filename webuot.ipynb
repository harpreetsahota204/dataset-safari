{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebUOT-1M dataset\n",
    "\n",
    "WebUOT-1M is the largest million-scale benchmark for underwater object tracking (UOT), designed to address limitations in existing datasets by providing diverse underwater scenarios, rich annotations, and language prompts. \n",
    "\n",
    "It comprises 1.1 million frames across 1,500 underwater videos, covering 408 target categories categorized into 12 superclasses (e.g., fish, molluscs, inanimate objects). The dataset includes high-quality bounding box annotations, 23 tracking attributes (e.g., illumination variation, camouflage), and language descriptions for multimodal tracking research.\n",
    "\n",
    "Note: This dataset, which has been parsed into FiftyOne format, comprises 238 randomly selected videos from the WebUOT-1M test set for a total of 192,000+ frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fiftyone umap-learn timm hiera-transformer einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "from fiftyone.utils.huggingface import load_from_hub\n",
    "\n",
    "dataset = load_from_hub(\n",
    "    \"Voxel51/WebUOT-238-Test\",\n",
    "    name=\"webuot238\",\n",
    "    overwrite=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the dataset has been downloaded you can begin exploring it in the FiftyOne app. Once the dataset has been downloaded, you can do some initial exploration by launching the app.\n",
    "\n",
    "There are two ways to use the app:\n",
    "\n",
    "1. As a cell in your notebook, which you can do by running:\n",
    "\n",
    "```python\n",
    "fo.launch_app(dataset)\n",
    "```\n",
    "\n",
    "2. In a separate browser window, run the following in your terminal:\n",
    "\n",
    "```bash\n",
    "fiftyone app launch\n",
    "```\n",
    "\n",
    "Once the app is launched, you can explore the dataset by:\n",
    "\n",
    "• Scrolling through the videos for a visual vibe check of its contents\n",
    "\n",
    "• Filter based on the labels (the various attributes associated with each video)\n",
    "\n",
    "• Filter based on the objects (the various ground truth labels)\n",
    "\n",
    "• Create a dashboard of plots for the various information fields of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Explore WebUOT](assets/explore-webuot.gif)\n",
    "\n",
    "\n",
    "## Exploring deeper\n",
    "\n",
    "We can gain a deeper understaning of this dataset by computing and visualizing embeddings for the videos.\n",
    "\n",
    "I've built a plugin which allows us to use the [Hiera embedding model](https://github.com/facebookresearch/hiera). FiftyOne's plugin framework lets you extend and customize the functionality of FiftyOne to suit your needs. If you’re interested in learning more about plugins, you might be interested in attending one of our monthly workshops. You can [see the full schedule here](https://voxel51.com/computer-vision-events/) and look for the Advanced Computer Vision Data Curation and Model Evaluation workshop.\n",
    "\n",
    "The [Hiera Embedding model](https://arxiv.org/abs/2306.00989) from Facebook is a hierarchical vision transformer for efficient image and video understanding tasks. It combines speed with high accuracy by simplifying traditional transformer architectures while maintaining performance through masked autoencoder (MAE) pretraining. This video embedding model was pretrained on the Kinetics-400 (K400) dataset. The masked autoencoder objective forces the model to learn robust spatiotemporal patterns by reconstructing randomly masked video patches. This video-specific pretraining enables temporal understanding capabilities, while still maintaining the core hierarchical architecture developed through image training.\n",
    "\n",
    "While not guaranteed, Hiera's embeddings frequently retain semantic value even for OOD data (like what we're working with) due to it's sparse token hierarchy and MAE's reconstruction-driven learning.\n",
    "\n",
    "The main point is that we can compute video embeddings [relatively easily with the plugin](https://github.com/harpreetsahota204/hiera-video-embeddings-plugin). Let's start by downloading the plugin and installing it's necessary dependencies.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download https://github.com/harpreetsahota204/hiera-video-embeddings-plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins requirements @harpreetsahota/hiera_video_embeddings --install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to set an enviornment variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['FIFTYONE_ALLOW_LEGACY_ORCHESTRATORS'] = 'true'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " I’ll assume that you’re running this in a Jupyter notebook, in which case you can run the entire model on the dataset as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.operators as foo\n",
    "\n",
    "hiera_embeddings = foo.get_operator(\"@harpreetsahota/hiera_video_embeddings/compute_hiera_video_embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can use the app and fill out the operator form. I'll refer you to [the GitHub repo for the plugin](https://github.com/harpreetsahota204/hiera-video-embeddings-plugin) for more details.\n",
    "\n",
    "This plugin supports all currently released versions and checkpoints of the [Hiera Video Models collection](https://github.com/facebookresearch/hiera):\n",
    "\n",
    "    - `hiera_base_16x224`\n",
    "    - `hiera_base_plus_16x224`\n",
    "    - `hiera_large_16x224`\n",
    "    - `hiera_huge_16x224`\n",
    "\n",
    "It also two types of embeddings:\n",
    "\n",
    "- **Terminal Embedding (`terminal`)**: A 768-dimensional embedding vector derived from the final layer of the model. This represents the global semantic context of the video sequence. Can optionally be normalized.\n",
    "  \n",
    "- **Hierarchical Embedding (`hierarchical`)**: A 1440-dimensional embedding vector that concatenates features across all intermediate outputs (96+192+384+768 = 1440 dimensions). This captures multi-scale representations of the video content. **These embeddings cannot be normalized.**\n",
    "\n",
    "Sadly, the Hiera video embedding model struggles with long duration videos. We'll work with only short duration videos.  I'm not too familar with many video embedding models, but if you know of one that I should create a plugin that works well for longer duration videos for please let me know. Note: the [V-JEPA model](https://github.com/facebookresearch/jepa) for video embeddings is currently on the roadmap. \n",
    "\n",
    "Luckily, you can easily filter your dataset as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fiftyone import ViewField as F\n",
    "\n",
    "short_videos = dataset.filter_labels(\n",
    "    \"Length\", F(\"label\").is_in([\"short\"])\n",
    ").clone(name=\"short_videos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leaves us with 147 samples that we will work with going forward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(short_videos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the following cell, you'll need to kick off a delegated operation. You can do this by opening your terminal and running `fiftyone delegated launch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await hiera_embeddings(\n",
    "    short_videos,\n",
    "    model_name=\"hiera_base_plus_16x224\",\n",
    "    checkpoint=\"mae_k400\", #one of mae_k400 OR mae_k400_ft_k400\n",
    "    embedding_types=\"terminal\", #or hierarchical\n",
    "    emb_field=\"hiera_video_embeddings\",\n",
    "    normalize=True, #defaults to False, only works with `terminal` embeddings\n",
    "    delegate=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_videos.persistent = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_videos.reload()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we're at it, let's go ahead and compute embeddings for the video captions as well. For this we'll make use of [Jina Embeddings V3](https://huggingface.co/jinaai/jina-embeddings-v3):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from transformers import AutoModel\n",
    "\n",
    "jina_embeddings_model = AutoModel.from_pretrained(\n",
    "    \"jinaai/jina-embeddings-v3\", \n",
    "    trust_remote_code=True,\n",
    "    device_map = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run the model on our dataset and use the `seperation` task as it's suitable for visualizing clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in short_videos.iter_samples(autosave=True):\n",
    "    text_embeddings = jina_embeddings_model.encode(\n",
    "        sentences = [sample[\"language\"]], # model expects a list of strings\n",
    "        task=\"separation\"\n",
    "        )\n",
    "    sample[\"text_embeddings\"] = text_embeddings.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can compute a 2D representation of our high-dimensional embeddings using UMAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.brain as fob\n",
    "\n",
    "embedding_fields = [ \"hiera_video_embeddings\", \"text_embeddings\"]\n",
    "\n",
    "for fields in embedding_fields:\n",
    "    _fname = fields.split(\"_embeddings\")[0]\n",
    "    results = fob.compute_visualization(\n",
    "        short_videos,\n",
    "        embeddings=fields,\n",
    "        method=\"umap\",\n",
    "        brain_key=f\"{_fname}_viz\",\n",
    "        num_dims=2,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And from here we can visualize our embeddings in the app\n",
    "\n",
    "![Visualizing Embeddings](assets/webuot-viz-embeddings.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think an interesting next step is applying SAM2 to this subset of data and seeing how it performs. To do that, start by installing the required dependencies for SAM2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"git+https://github.com/facebookresearch/sam2.git#egg=sam-2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FiftyOne has an [integration with SAM2](https://voxel51.com/blog/sam-2-is-now-available-in-fiftyone/), and we can make use of that through the [FiftyOne Model Zoo](https://docs.voxel51.com/model_zoo/index.html#fiftyone-model-zoo). The model zoo gives provides you native access to hundreds of pre-trained models. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import fiftyone.zoo as foz\n",
    "\n",
    "sam_model = foz.load_zoo_model(\n",
    "    \"segment-anything-2-hiera-tiny-video-torch\",\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAM2 (Segment Anything Model 2) offers powerful video segmentation capabilities. \n",
    "\n",
    "Its key features include:\n",
    "\n",
    "1. Precise object segmentation and tracking across video frames\n",
    "2. Simple prompting methods:\n",
    "   - Bounding boxes\n",
    "   - Point selections\n",
    "3. Efficient workflow:\n",
    "   - Only requires prompts on the first frame\n",
    "   - Automatically propagates segmentation masks to subsequent frames\n",
    "\n",
    "This means we can identify an object in the first frame of a video, and SAM2 will automatically track and segment that object throughout the entire sequence.\n",
    "\n",
    "\n",
    "Once you've instantiated the model, the next step is to apply it to your dataset. Note that depending on the type of GPU you're running this on, it can take quite a bit of time. For reference, I ran this on an NVIDIA RTX 6000 Ada and it took a little over an hour. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_videos.apply_model(\n",
    "    sam_model,\n",
    "    label_field=\"sam_segmentations\",\n",
    "    prompt_field=\"frames.gt\", # Can be a detections or a keypoint field\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model has been applied to the dataset, we can look at the results in the app for a heuristics driven visual vibe check of model performance.\n",
    "\n",
    "From an initial visual inspection it seems like SAM2 does a fairly good job of segmenting the objects of interest. There are some cases where the masks aren't as tight, but given the fact that this is an underwater dataset it's still quite impressive.\n",
    "\n",
    "![SAM2 Predictions](assets/webuot-sam2-preds.gif)\n",
    "\n",
    "However, what's more impressive, at least from my initial visual vibe check, is the quality of the bounding boxes generated by SAM2. It seems the boxes are on point with, and at times tighter than, the ground truth boxes!\n",
    "\n",
    "Of course, we can perform a more rigorous evaluation using the [`evaluate_detections`](https://docs.voxel51.com/tutorials/evaluate_detections.html#Evaluating-Object-Detections-with-FiftyOne) method of the dataset and get some concrete number for model performance. Since the dataset doesn't have ground truth annotations for segmentation masks, we can peform evaluation of the predicted bounding boxes against the ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_videos.evaluate_detections(\n",
    "    pred_field=\"frames.sam_segmentations\",\n",
    "    gt_field=\"frames.gt\",\n",
    "    eval_key=\"sam_eval\",\n",
    "    iou=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can analyze the results of the evaluation right in the app via the Model Evaluation panel:\n",
    "\n",
    "![SAM2 Model Eval](assets/webuot-model-eval.gif)\n",
    "\n",
    "### An important consideration\n",
    "\n",
    "In this demonstration, we're using SAM2 to showcase basic segmentation capabilities on underwater footage, focusing primarily on the spatial accuracy of masks and bounding boxes.  For this simplified use case, we'll evaluate the model using IoU (Intersection over Union) metrics to assess how well SAM2 can identify objects frame by frame. \n",
    "\n",
    "However, it's important to note that real-world underwater object tracking presents significantly more complex challenges. \n",
    "\n",
    "While this SAM2 demonstration shows promising results for basic segmentation and bounding box tracking,a complete tracking solution would need more sophisticated components to handle these advanced tracking requirements. To illustrate the concretely, suppose we're concerned with tracking fish, then we need to consider (though the same considerations apply to any object tracking task):\n",
    "\n",
    "   - **Identity Preservation**: Maintaining track of a specific fish among similar-looking ones. For example, when tracking a particular clownfish in a group, the system must maintain its unique identity even when other clownfish cross its path.\n",
    "\n",
    "   - **Distractor Handling**: Not getting confused by other fish of the same species. The system needs to distinguish the target from visually similar fish that may enter the frame, even when they exhibit similar patterns or behaviors.\n",
    "\n",
    "   - **Temporal Consistency**: Maintaining the same ID across frames. This involves predicting motion patterns and understanding typical fish behaviors to maintain tracking even during quick movements or direction changes.\n",
    "\n",
    "   - **Re-identification**: Recognizing the same fish after temporary occlusion. When the target fish temporarily disappears behind coral or other fish, the system must be able to recognize and re-establish tracking when it reappears.\n",
    "\n",
    "   - **Group Behavior Handling**: Managing scenarios where fish (or other marine life) move in schools or groups, need more sophistication to maintain individual tracking within collective movement patterns.\n",
    "\n",
    "\n",
    "So while SAM2 is great for demonstrating segmentation capabilities, a production underwater tracking system would need additional components to handle the complex identity tracking challenges. Check out [this blog](https://voxel51.com/blog/tracking-datasets-in-fiftyone/) for more detail.\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "In this exploration of the WebUOTdataset, we've demonstrated several key capabilities:\n",
    "\n",
    "1. Dataset visualization and exploration using FiftyOne\n",
    "\n",
    "2. Computing and visualizing video embeddings using the Hiera model\n",
    "\n",
    "3. Generating text embeddings using Jina Embeddings V3\n",
    "\n",
    "4. Applying SAM2 for object segmentation and detection\n",
    "\n",
    "Our evaluation of SAM2's performance on underwater footage shows promising results for basic segmentation tasks. However, this demonstration only scratches the surface of what's needed for comprehensive underwater object tracking. Real-world applications require sophisticated systems that can handle identity preservation, temporal consistency, occlusion recovery, and group dynamics. \n",
    "\n",
    "These challenges are particularly acute in underwater environments where factors like variable visibility, light refraction, and complex marine life behaviors come into play.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "fiftyone"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
